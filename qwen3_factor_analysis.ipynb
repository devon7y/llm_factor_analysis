{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Factor Analysis - Personality Items (Qwen3-Embedding Models)\n",
    "\n",
    "Extracts embeddings from NEO personality items using **Qwen3-Embedding models** and compares predicted similarities with observed correlations.\n",
    "\n",
    "**Available Models:**\n",
    "- Qwen3-Embedding-0.6B (600M parameters, ~2.4 GB)\n",
    "- Qwen3-Embedding-4B (4B parameters, ~16 GB) \n",
    "- Qwen3-Embedding-8B (8B parameters, ~32 GB)\n",
    "\n",
    "**Model Configuration:**\n",
    "- Precision: FP32 (full precision)\n",
    "- Library: sentence-transformers (simplified API)\n",
    "- Supports: 100+ languages, MTEB top-ranked performance\n",
    "\n",
    "**Note:** You can easily enable/disable specific models in the model selection cell below. By default, all three models are enabled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies\n",
    "\n",
    "This notebook requires:\n",
    "- `sentence-transformers>=2.7.0`\n",
    "- `transformers>=4.51.0`\n",
    "- `torch>=2.0.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport sys\nfrom datetime import datetime\n\nimport pandas as pd\nimport numpy as np\nprint(\"  - pandas and numpy loaded\")\n\nimport torch\nprint(f\"  - torch {torch.__version__} loaded\")\n\n# Check if sentence-transformers is installed\ntry:\n    from sentence_transformers import SentenceTransformer\n    import sentence_transformers\n    print(f\"  - sentence-transformers {sentence_transformers.__version__} loaded\")\nexcept ImportError:\n    print(\"\\nERROR: sentence-transformers not found!\")\n    print(\"Please install: pip install sentence-transformers>=2.7.0\")\n    raise\n\n# Check transformers version\nimport transformers\nprint(f\"  - transformers {transformers.__version__} loaded\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading scale...\")\n",
    "scale = pd.read_csv('DASS_items.csv', usecols=['code', 'item', 'factor'])\n",
    "print(f\"Loaded {len(scale)} items\")\n",
    "\n",
    "# Preview the data\n",
    "scale.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract codes, items and factors for easier access\n",
    "codes = scale['code'].tolist()\n",
    "items = scale['item'].tolist()\n",
    "factors = scale['factor'].tolist()\n",
    "\n",
    "print(f\"Total items: {len(items)}\")\n",
    "print(f\"Unique factors: {sorted(set(factors))}\")\n",
    "print(f\"Sample codes: {codes[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Detection and Memory Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect available device\n",
    "print(\"Detecting available device...\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"✓ Using CUDA GPU: {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(\"✓ Using Apple MPS GPU (Metal Performance Shaders)\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"✓ Using CPU\")\n",
    "\n",
    "# Check available memory (basic check)\n",
    "try:\n",
    "    import psutil\n",
    "    mem = psutil.virtual_memory()\n",
    "    print(f\"\\nSystem Memory:\")\n",
    "    print(f\"  Total: {mem.total / (1024**3):.1f} GB\")\n",
    "    print(f\"  Available: {mem.available / (1024**3):.1f} GB\")\n",
    "    print(f\"  Used: {mem.percent}%\")\n",
    "except ImportError:\n",
    "    print(\"\\n  psutil not installed - skipping memory check\")\n",
    "    print(\"Install with: pip install psutil\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Qwen3-Embedding Models\n",
    "\n",
    "**Memory Requirements (FP32 full precision):**\n",
    "- Qwen3-Embedding-0.6B: ~2.4 GB\n",
    "- Qwen3-Embedding-4B: ~16 GB\n",
    "- Qwen3-Embedding-8B: ~32 GB\n",
    "\n",
    "We'll load and process each model sequentially to analyze how model size affects embedding quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 0.6B model loaded successfully!\n",
      "  Device: mps:0\n",
      "  Max sequence length: 32768\n",
      "  Embedding dimension: 1024\n",
      "\n",
      "======================================================================\n",
      "✓ Successfully loaded 1 model(s)!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MODEL SELECTION - Comment out any models you don't want to run\n",
    "# ============================================================================\n",
    "# To disable a model, add a '#' at the start of its line\n",
    "# To enable a model, remove the '#' from the start of its line\n",
    "# ============================================================================\n",
    "\n",
    "model_names = [\n",
    "    \"Qwen/Qwen3-Embedding-0.6B\",\n",
    "    #\"Qwen/Qwen3-Embedding-4B\", \n",
    "    #\"Qwen/Qwen3-Embedding-8B\"\n",
    "]\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "# Dictionary to store results\n",
    "all_embeddings = {}\n",
    "all_models = {}\n",
    "\n",
    "print(f\"Loading {len(model_names)} Qwen3-Embedding model(s)...\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Selected models: {[m.split('/')[-1] for m in model_names]}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for model_name in model_names:\n",
    "    model_size = model_name.split(\"-\")[-1]\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Loading {model_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"This may take 30-120 seconds depending on model size...\")\n",
    "    \n",
    "    try:\n",
    "        # Load model - let sentence-transformers handle device placement automatically\n",
    "        # Don't use device_map=\"auto\" as it causes meta tensor issues with large models\n",
    "        model = SentenceTransformer(model_name)\n",
    "        \n",
    "        print(f\"✓ {model_size} model loaded successfully!\")\n",
    "        print(f\"  Device: {model.device}\")\n",
    "        print(f\"  Max sequence length: {model.max_seq_length}\")\n",
    "        \n",
    "        # Check embedding dimension\n",
    "        test_embedding = model.encode([\"test\"], convert_to_numpy=True)\n",
    "        print(f\"  Embedding dimension: {test_embedding.shape[1]}\")\n",
    "        \n",
    "        # Store the model for later use\n",
    "        all_models[model_size] = model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError loading {model_name}:\")\n",
    "        print(f\"  {type(e).__name__}: {str(e)}\")\n",
    "        print(f\"  Skipping this model and continuing with others...\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"✓ Successfully loaded {len(all_models)} model(s)!\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Embeddings for All Models\n",
    "\n",
    "Using sentence-transformers' `encode()` method for each model.\n",
    "\n",
    "**Processing:**\n",
    "- Extract embeddings for all personality items with each model\n",
    "- Using `batch_size=8` for efficient processing\n",
    "- Results stored in `all_embeddings` dictionary keyed by model size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting embeddings for 42 personality items using all models...\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Processing with 0.6B model\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 6/6 [00:17<00:00,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Embedding extraction complete for 0.6B!\n",
      "  Shape: (42, 1024)\n",
      "  (42 items × 1024 dimensions)\n",
      "\n",
      "======================================================================\n",
      "✓ All embeddings extracted successfully!\n",
      "======================================================================\n",
      "\n",
      "Embedding dimensions by model:\n",
      "  0.6B: 1024D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Extracting embeddings for {len(items)} personality items using all models...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for model_size, model in all_models.items():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Processing with {model_size} model\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    try:\n",
    "        # Extract embeddings\n",
    "        embeddings = model.encode(\n",
    "            items,\n",
    "            batch_size=8,  # Process in batches\n",
    "            show_progress_bar=True,\n",
    "            convert_to_numpy=True,\n",
    "            normalize_embeddings=False  # Keep raw embeddings for analysis\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        all_embeddings[model_size] = embeddings\n",
    "        \n",
    "        print(f\"\\n✓ Embedding extraction complete for {model_size}!\")\n",
    "        print(f\"  Shape: {embeddings.shape}\")\n",
    "        print(f\"  ({embeddings.shape[0]} items × {embeddings.shape[1]} dimensions)\")\n",
    "        \n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e).lower():\n",
    "            print(f\"\\nOut of memory error with {model_size}!\")\n",
    "            print(f\"  Try reducing batch_size or using a smaller model\")\n",
    "            raise\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"✓ All embeddings extracted successfully!\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\nEmbedding dimensions by model:\")\n",
    "for model_size, embeddings in all_embeddings.items():\n",
    "    print(f\"  {model_size}: {embeddings.shape[1]}D\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Embedding Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimensions by model:\n",
      "======================================================================\n",
      "\n",
      "0.6B:\n",
      "  Shape: (42, 1024)\n",
      "  Embedding dimension: 1024D\n",
      "  First embedding (first 10 values): [ 0.01202668 -0.0104402  -0.00594882  0.03350148  0.01650581 -0.02499348\n",
      "  0.02960537 -0.03747144 -0.07596644  0.06410423]\n"
     ]
    }
   ],
   "source": [
    "# Compare embedding dimensions across models\n",
    "print(\"Embedding dimensions by model:\")\n",
    "print(\"=\" * 70)\n",
    "for model_size, embeddings in all_embeddings.items():\n",
    "    print(f\"\\n{model_size}:\")\n",
    "    print(f\"  Shape: {embeddings.shape}\")\n",
    "    print(f\"  Embedding dimension: {embeddings.shape[1]}D\")\n",
    "    print(f\"  First embedding (first 10 values): {embeddings[0][:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding statistics by model:\n",
      "======================================================================\n",
      "\n",
      "0.6B:\n",
      "  Min value: -0.1468\n",
      "  Max value: 0.1739\n",
      "  Mean: -0.0004\n",
      "  Std: 0.0312\n"
     ]
    }
   ],
   "source": [
    "# Summary statistics for all models\n",
    "print(\"Embedding statistics by model:\")\n",
    "print(\"=\" * 70)\n",
    "for model_size, embeddings in all_embeddings.items():\n",
    "    print(f\"\\n{model_size}:\")\n",
    "    print(f\"  Min value: {embeddings.min():.4f}\")\n",
    "    print(f\"  Max value: {embeddings.max():.4f}\")\n",
    "    print(f\"  Mean: {embeddings.mean():.4f}\")\n",
    "    print(f\"  Std: {embeddings.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample item #0:\n",
      "  Factor: Stress\n",
      "  Text: I found myself getting upset by quite trivial things.\n",
      "\n",
      "Embedding properties by model:\n",
      "======================================================================\n",
      "\n",
      "0.6B:\n",
      "  Embedding shape: (1024,)\n",
      "  Embedding norm: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Check a specific item across all models\n",
    "sample_idx = 0\n",
    "print(f\"Sample item #{sample_idx}:\")\n",
    "print(f\"  Factor: {factors[sample_idx]}\")\n",
    "print(f\"  Text: {items[sample_idx]}\")\n",
    "print(\"\\nEmbedding properties by model:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for model_size, embeddings in all_embeddings.items():\n",
    "    print(f\"\\n{model_size}:\")\n",
    "    print(f\"  Embedding shape: {embeddings[sample_idx].shape}\")\n",
    "    print(f\"  Embedding norm: {np.linalg.norm(embeddings[sample_idx]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-SNE Visualization - All Models\n",
    "\n",
    "Visualize the high-dimensional embeddings in 2D space using T-SNE, color-coded by personality factor.\n",
    "\n",
    "We'll create separate T-SNE plots for each model to compare how different model sizes capture semantic relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization libraries loaded\n"
     ]
    }
   ],
   "source": [
    "# Import visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "print(\"Visualization libraries loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for T-SNE...\n",
      "Number of items: 42\n",
      "Personality factors: ['Anxiety', 'Depression', 'Stress']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tw/9fmk8_cs08v70918my8dtdmh0000gn/T/ipykernel_1727/4078075515.py:11: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  colors_map = cm.get_cmap('tab10', len(unique_factors))\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for T-SNE (same across all models)\n",
    "print(\"Preparing data for T-SNE...\")\n",
    "print(f\"Number of items: {len(factors)}\")\n",
    "\n",
    "# Get unique factors for legend\n",
    "unique_factors = sorted(set(factors))\n",
    "print(f\"Personality factors: {unique_factors}\")\n",
    "\n",
    "# Create a color map for the personality factors\n",
    "import matplotlib.cm as cm\n",
    "colors_map = cm.get_cmap('tab10', len(unique_factors))\n",
    "factor_to_color = {factor: colors_map(i) for i, factor in enumerate(unique_factors)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running T-SNE for all models...\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Running T-SNE for 0.6B model...\n",
      "======================================================================\n",
      "Input shape: (42, 1024)\n",
      "[t-SNE] Computing 41 nearest neighbors...\n",
      "[t-SNE] Indexed 42 samples in 0.053s...\n",
      "[t-SNE] Computed neighbors for 42 samples in 0.599s...\n",
      "[t-SNE] Computed conditional probabilities for sample 42 / 42\n",
      "[t-SNE] Mean sigma: 0.424035\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 52.968773\n",
      "[t-SNE] KL divergence after 1000 iterations: 0.135373\n",
      "✓ T-SNE complete! 2D embeddings shape: (42, 2)\n",
      "\n",
      "======================================================================\n",
      "✓ T-SNE complete for all 1 models!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Run T-SNE and create visualizations for all models\n",
    "print(\"Running T-SNE for all models...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "all_tsne_embeddings = {}\n",
    "\n",
    "for model_size, embeddings in all_embeddings.items():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Running T-SNE for {model_size} model...\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Input shape: {embeddings.shape}\")\n",
    "    \n",
    "    # Run T-SNE dimensionality reduction\n",
    "    tsne = TSNE(\n",
    "        n_components=2,      # Reduce to 2D\n",
    "        perplexity=25,       # Balance local vs global structure\n",
    "        max_iter=1000,       # Number of iterations\n",
    "        random_state=42,     # For reproducibility\n",
    "        verbose=1            # Show progress\n",
    "    )\n",
    "    \n",
    "    # Transform high-D embeddings to 2D\n",
    "    embeddings_2d = tsne.fit_transform(embeddings)\n",
    "    all_tsne_embeddings[model_size] = embeddings_2d\n",
    "    \n",
    "    print(f\"✓ T-SNE complete! 2D embeddings shape: {embeddings_2d.shape}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"✓ T-SNE complete for all {len(all_tsne_embeddings)} models!\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create T-SNE scatter plots for all models\nprint(\"Creating visualizations...\")\nprint(\"=\" * 70)\n\n# Create plots directory if it doesn't exist\nplots_dir = \"plots\"\nos.makedirs(plots_dir, exist_ok=True)\nprint(f\"Plots will be saved to: {plots_dir}/\")\n\n# Generate timestamp for filename\ntimestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n\n# Determine number of models\nnum_models = len(all_tsne_embeddings)\nprint(f\"Creating plots for {num_models} model(s)...\")\n\n# Check if we have any models to plot\nif num_models == 0:\n    print(\"\\n⚠ WARNING: No T-SNE embeddings available to plot!\")\n    print(\"  This could mean:\")\n    print(\"  - No models were successfully loaded\")\n    print(\"  - Embedding extraction failed for all models\")\n    print(\"  - T-SNE failed for all models\")\n    print(\"\\nSkipping visualization...\")\nelse:\n    # Adjust figure size based on number of models\n    # Use dynamic sizing, with 3 columns max for better layout\n    fig_width = min(24, 8 * num_models)  # Cap at 24 inches\n    fig, axes = plt.subplots(1, min(3, num_models), figsize=(fig_width, 8))\n    \n    # Handle case of single model (axes is not a list in this case)\n    if min(3, num_models) == 1:\n        axes = [axes]\n    \n    for idx, (model_size, embeddings_2d) in enumerate(sorted(all_tsne_embeddings.items())):\n        if idx >= 3:  # Only plot first 3 models in this cell\n            break\n            \n        ax = axes[idx]\n        \n        # Plot each factor with a different color\n        for factor in unique_factors:\n            # Get indices for this factor\n            indices = [i for i, f in enumerate(factors) if f == factor]\n            \n            # Plot points for this factor\n            ax.scatter(\n                embeddings_2d[indices, 0],\n                embeddings_2d[indices, 1],\n                c=[factor_to_color[factor]],\n                label=factor,\n                alpha=0.6,\n                s=80,\n                edgecolors='white',\n                linewidth=0.5\n            )\n        \n        # Add labels for each point using the 'code' column\n        for i in range(len(embeddings_2d)):\n            ax.annotate(\n                codes[i],  # Use the code as the label\n                (embeddings_2d[i, 0], embeddings_2d[i, 1]),\n                fontsize=7,\n                alpha=0.7,\n                ha='center',\n                va='bottom',\n                xytext=(0, 3),  # Offset label slightly above the point\n                textcoords='offset points'\n            )\n        \n        # Get embedding dimension for title\n        embedding_dim = all_embeddings[model_size].shape[1]\n        \n        ax.set_xlabel('T-SNE Component 1', fontsize=11)\n        ax.set_ylabel('T-SNE Component 2', fontsize=11)\n        ax.set_title(\n            f'{model_size} Model\\n({embedding_dim}D → 2D)',\n            fontsize=13,\n            fontweight='bold'\n        )\n        ax.grid(True, alpha=0.3, linestyle='--')\n        \n        # Only add legend to the rightmost plot\n        if idx == min(2, num_models - 1):\n            ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n    \n    # Overall title\n    fig.suptitle(\n        'T-SNE Visualization of Scale Item Embeddings\\n'\n        'Qwen3-Embedding Models',\n        fontsize=16,\n        fontweight='bold',\n        y=1.00\n    )\n    \n    plt.tight_layout()\n    \n    # Save the figure\n    model_names_str = \"_\".join(sorted(list(all_tsne_embeddings.keys())[:3]))\n    filename = f\"qwen3_tsne_visualization_{model_names_str}_{timestamp}.png\"\n    filepath = os.path.join(plots_dir, filename)\n    plt.savefig(filepath, dpi=300, bbox_inches='tight')\n    print(f\"\\n✓ Plot saved to: {filepath}\")\n    \n    # Display the plot\n    plt.show()\n    \n    print(\"\\n✓ Visualization complete!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Nearest Neighbors - All Models\n",
    "\n",
    "Compare how different model sizes identify semantic neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create T-SNE scatter plots for all models\nprint(\"Creating visualizations...\")\nprint(\"=\" * 70)\n\n# Create plots directory if it doesn't exist\nplots_dir = \"plots\"\nos.makedirs(plots_dir, exist_ok=True)\nprint(f\"Plots will be saved to: {plots_dir}/\")\n\n# Generate timestamp for filename\ntimestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n\n# Determine number of models and create appropriate figure\nnum_models = len(all_tsne_embeddings)\nprint(f\"Creating plots for {num_models} model(s)...\")\n\n# Check if we have any models to plot\nif num_models == 0:\n    print(\"\\n⚠ WARNING: No T-SNE embeddings available to plot!\")\n    print(\"  This could mean:\")\n    print(\"  - No models were successfully loaded\")\n    print(\"  - Embedding extraction failed for all models\")\n    print(\"  - T-SNE failed for all models\")\n    print(\"\\nSkipping visualization...\")\nelse:\n    # Adjust figure size based on number of models\n    fig_width = 8 * num_models  # 8 inches per model\n    fig, axes = plt.subplots(1, num_models, figsize=(fig_width, 8))\n    \n    # Handle case of single model (axes is not a list in this case)\n    if num_models == 1:\n        axes = [axes]\n    \n    for idx, (model_size, embeddings_2d) in enumerate(sorted(all_tsne_embeddings.items())):\n        ax = axes[idx]\n        \n        # Plot each factor with a different color\n        for factor in unique_factors:\n            # Get indices for this factor\n            indices = [i for i, f in enumerate(factors) if f == factor]\n            \n            # Plot points for this factor\n            ax.scatter(\n                embeddings_2d[indices, 0],\n                embeddings_2d[indices, 1],\n                c=[factor_to_color[factor]],\n                label=factor,\n                alpha=0.6,\n                s=80,\n                edgecolors='white',\n                linewidth=0.5\n            )\n        \n        # Add labels for each point using the 'code' column\n        for i in range(len(embeddings_2d)):\n            ax.annotate(\n                codes[i],  # Use the code as the label\n                (embeddings_2d[i, 0], embeddings_2d[i, 1]),\n                fontsize=7,\n                alpha=0.7,\n                ha='center',\n                va='bottom',\n                xytext=(0, 3),  # Offset label slightly above the point\n                textcoords='offset points'\n            )\n        \n        # Get embedding dimension for title\n        embedding_dim = all_embeddings[model_size].shape[1]\n        \n        ax.set_xlabel('T-SNE Component 1', fontsize=11)\n        ax.set_ylabel('T-SNE Component 2', fontsize=11)\n        ax.set_title(\n            f'{model_size} Model\\n({embedding_dim}D → 2D)',\n            fontsize=13,\n            fontweight='bold'\n        )\n        ax.grid(True, alpha=0.3, linestyle='--')\n        \n        # Add legend to the rightmost plot\n        if idx == num_models - 1:\n            ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n    \n    # Overall title\n    fig.suptitle(\n        'T-SNE Visualization of NEO Personality Item Embeddings\\n'\n        f'Qwen3-Embedding Models (FP32, 50 items) - {num_models} model(s)',\n        fontsize=16,\n        fontweight='bold',\n        y=1.00\n    )\n    \n    plt.tight_layout()\n    \n    # Save the figure\n    model_names_str = \"_\".join(sorted(all_tsne_embeddings.keys()))\n    filename = f\"qwen3_tsne_visualization_detailed_{model_names_str}_{timestamp}.png\"\n    filepath = os.path.join(plots_dir, filename)\n    plt.savefig(filepath, dpi=300, bbox_inches='tight')\n    print(f\"\\n✓ Plot saved to: {filepath}\")\n    \n    # Display the plot\n    plt.show()\n    \n    print(\"\\n✓ Visualization complete!\")"
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding nearest neighbors in original embedding space (not T-SNE)...\n",
      "======================================================================\n",
      "\n",
      "Sample item #0:\n",
      "  Factor: Stress\n",
      "  Text: I found myself getting upset by quite trivial things.\n",
      "\n",
      "======================================================================\n",
      "0.6B Model - Original 1024D Space\n",
      "======================================================================\n",
      "5 Most similar items (by cosine similarity):\n",
      "  1. [Stress] I found myself getting upset rather easily.\n",
      "      Similarity: 0.8722\n",
      "  2. [Stress] I found myself getting agitated.\n",
      "      Similarity: 0.7611\n",
      "  3. [Stress] I found that I was very irritable.\n",
      "      Similarity: 0.7597\n",
      "  4. [Stress] I found it hard to calm down after something upset me.\n",
      "      Similarity: 0.7579\n",
      "  5. [Stress] I felt that I was rather touchy.\n",
      "      Similarity: 0.7467\n"
     ]
    }
   ],
   "source": [
    "# Analyze nearest neighbors in the ORIGINAL high-dimensional space for all models\n",
    "print(\"Finding nearest neighbors in original embedding space (not T-SNE)...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(f\"\\nSample item #{sample_idx}:\")\n",
    "print(f\"  Factor: {factors[sample_idx]}\")\n",
    "print(f\"  Text: {items[sample_idx]}\")\n",
    "\n",
    "for model_size, embeddings in sorted(all_embeddings.items()):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"{model_size} Model - Original {embeddings.shape[1]}D Space\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Compute cosine similarity between sample and all items\n",
    "    similarities = cosine_similarity([embeddings[sample_idx]], embeddings)[0]\n",
    "    \n",
    "    # Find 5 most similar items (excluding itself)\n",
    "    most_similar_indices = np.argsort(similarities)[::-1][1:6]\n",
    "    \n",
    "    print(f\"5 Most similar items (by cosine similarity):\")\n",
    "    for rank, idx in enumerate(most_similar_indices, 1):\n",
    "        print(f\"  {rank}. [{factors[idx]}] {items[idx]}\")\n",
    "        print(f\"      Similarity: {similarities[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantify Factor Separation\n",
    "\n",
    "Measure how well the embeddings separate the three DASS factors (Anxiety, Depression, Stress) using cosine similarity analysis.\n",
    "\n",
    "**Metrics:**\n",
    "- **Within-factor similarity**: Average cosine similarity between items in the same factor\n",
    "- **Between-factor similarity**: Average cosine similarity between items in different factors  \n",
    "- **Separation ratio**: Within / Between (higher = better separation, >1.0 means factors cluster together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing factor separation metrics...\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "0.6B Model - Factor Separation Analysis\n",
      "======================================================================\n",
      "\n",
      "Overall Separation Metrics:\n",
      "  Within-factor similarity:  0.6528\n",
      "  Between-factor similarity: 0.5538\n",
      "  Separation ratio:          1.1788\n",
      "    (Good separation - factors cluster together!)\n",
      "\n",
      "Per-Factor Within-Similarity:\n",
      "  Anxiety     : 0.6009 ± 0.0940  (n=91 pairs)\n",
      "  Depression  : 0.6577 ± 0.0963  (n=91 pairs)\n",
      "  Stress      : 0.6998 ± 0.0778  (n=91 pairs)\n",
      "\n",
      "Between-Factor Similarities:\n",
      "  Anxiety      vs Depression  : 0.5140 ± 0.0640  (n=196 pairs)\n",
      "  Anxiety      vs Stress      : 0.5814 ± 0.0768  (n=196 pairs)\n",
      "  Depression   vs Stress      : 0.5659 ± 0.0703  (n=196 pairs)\n",
      "\n",
      "======================================================================\n",
      "Factor separation analysis complete!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing factor separation metrics...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Loop through all models\n",
    "for model_size, embeddings in sorted(all_embeddings.items()):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"{model_size} Model - Factor Separation Analysis\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Compute full similarity matrix\n",
    "    sim_matrix = cosine_similarity(embeddings)\n",
    "    \n",
    "    # Initialize accumulators\n",
    "    within_factor_sims = {factor: [] for factor in unique_factors}\n",
    "    between_factor_sims = []\n",
    "    \n",
    "    # Compute within-factor and between-factor similarities\n",
    "    for i in range(len(items)):\n",
    "        for j in range(i + 1, len(items)):  # Only upper triangle (avoid duplicates)\n",
    "            similarity = sim_matrix[i, j]\n",
    "            \n",
    "            if factors[i] == factors[j]:\n",
    "                # Same factor - within-factor similarity\n",
    "                within_factor_sims[factors[i]].append(similarity)\n",
    "            else:\n",
    "                # Different factors - between-factor similarity\n",
    "                between_factor_sims.append(similarity)\n",
    "    \n",
    "    # Compute overall metrics\n",
    "    all_within_sims = []\n",
    "    for factor_sims in within_factor_sims.values():\n",
    "        all_within_sims.extend(factor_sims)\n",
    "    \n",
    "    within_mean = np.mean(all_within_sims)\n",
    "    between_mean = np.mean(between_factor_sims)\n",
    "    separation_ratio = within_mean / between_mean\n",
    "    \n",
    "    # Print overall results\n",
    "    print(f\"\\nOverall Separation Metrics:\")\n",
    "    print(f\"  Within-factor similarity:  {within_mean:.4f}\")\n",
    "    print(f\"  Between-factor similarity: {between_mean:.4f}\")\n",
    "    print(f\"  Separation ratio:          {separation_ratio:.4f}\")\n",
    "    print(f\"    {'(Good separation - factors cluster together!)' if separation_ratio > 1.0 else '(Poor separation - factors overlap)'}\")\n",
    "    \n",
    "    # Print per-factor breakdown\n",
    "    print(f\"\\nPer-Factor Within-Similarity:\")\n",
    "    for factor in unique_factors:\n",
    "        factor_mean = np.mean(within_factor_sims[factor])\n",
    "        factor_std = np.std(within_factor_sims[factor])\n",
    "        n_pairs = len(within_factor_sims[factor])\n",
    "        print(f\"  {factor:12s}: {factor_mean:.4f} ± {factor_std:.4f}  (n={n_pairs} pairs)\")\n",
    "    \n",
    "    # Compute pairwise between-factor similarities\n",
    "    print(f\"\\nBetween-Factor Similarities:\")\n",
    "    factor_pairs = {}\n",
    "    for i in range(len(items)):\n",
    "        for j in range(i + 1, len(items)):\n",
    "            if factors[i] != factors[j]:\n",
    "                pair = tuple(sorted([factors[i], factors[j]]))\n",
    "                if pair not in factor_pairs:\n",
    "                    factor_pairs[pair] = []\n",
    "                factor_pairs[pair].append(sim_matrix[i, j])\n",
    "    \n",
    "    for pair in sorted(factor_pairs.keys()):\n",
    "        pair_mean = np.mean(factor_pairs[pair])\n",
    "        pair_std = np.std(factor_pairs[pair])\n",
    "        n_pairs = len(factor_pairs[pair])\n",
    "        print(f\"  {pair[0]:12s} vs {pair[1]:12s}: {pair_mean:.4f} ± {pair_std:.4f}  (n={n_pairs} pairs)\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"Factor separation analysis complete!\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Factor Centroids\n",
    "\n",
    "Compute the mean embedding (centroid) for each of the three DASS factors. These centroids represent the \"average\" embedding for each psychological dimension and can be used for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating factor centroids...\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "0.6B Model - Computing Centroids\n",
      "======================================================================\n",
      "\n",
      "Anxiety:\n",
      "  Number of items: 14\n",
      "  Centroid shape: (1024,)\n",
      "  Centroid norm: 0.7933\n",
      "\n",
      "Depression:\n",
      "  Number of items: 14\n",
      "  Centroid shape: (1024,)\n",
      "  Centroid norm: 0.8259\n",
      "\n",
      "Stress:\n",
      "  Number of items: 14\n",
      "  Centroid shape: (1024,)\n",
      "  Centroid norm: 0.8492\n",
      "\n",
      "======================================================================\n",
      "Centroid calculation complete!\n",
      "======================================================================\n",
      "\n",
      "Centroids stored in 'all_centroids' dictionary:\n",
      "  Structure: all_centroids[model_size][factor] = centroid_vector\n",
      "  Models: ['0.6B']\n",
      "  Factors per model: ['Anxiety', 'Depression', 'Stress']\n"
     ]
    }
   ],
   "source": [
    "print(\"Calculating factor centroids...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Dictionary to store centroids for all models\n",
    "all_centroids = {}\n",
    "\n",
    "# Loop through all models\n",
    "for model_size, embeddings in sorted(all_embeddings.items()):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"{model_size} Model - Computing Centroids\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Initialize centroid dictionary for this model\n",
    "    centroids = {}\n",
    "    \n",
    "    # Calculate centroid (mean embedding) for each factor\n",
    "    for factor in unique_factors:\n",
    "        # Get indices of items belonging to this factor\n",
    "        factor_indices = [i for i, f in enumerate(factors) if f == factor]\n",
    "        \n",
    "        # Get embeddings for this factor\n",
    "        factor_embeddings = embeddings[factor_indices]\n",
    "        \n",
    "        # Compute centroid (mean of all embeddings in this factor)\n",
    "        centroid = np.mean(factor_embeddings, axis=0)\n",
    "        \n",
    "        # Store centroid\n",
    "        centroids[factor] = centroid\n",
    "        \n",
    "        # Print info\n",
    "        print(f\"\\n{factor}:\")\n",
    "        print(f\"  Number of items: {len(factor_indices)}\")\n",
    "        print(f\"  Centroid shape: {centroid.shape}\")\n",
    "        print(f\"  Centroid norm: {np.linalg.norm(centroid):.4f}\")\n",
    "    \n",
    "    # Store centroids for this model\n",
    "    all_centroids[model_size] = centroids\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"Centroid calculation complete!\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\nCentroids stored in 'all_centroids' dictionary:\")\n",
    "print(f\"  Structure: all_centroids[model_size][factor] = centroid_vector\")\n",
    "print(f\"  Models: {list(all_centroids.keys())}\")\n",
    "print(f\"  Factors per model: {list(all_centroids[list(all_centroids.keys())[0]].keys())}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}