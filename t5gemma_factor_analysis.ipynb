{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# LLM Factor Analysis - Personality Items (T5-GEMMA Encoder)\n",
    "\n",
    "Extracts embeddings from DASS personality items using **T5-GEMMA encoder** and compares predicted similarities with observed correlations.\n",
    "\n",
    "**Available Models:**\n",
    "- T5-GEMMA-Base (~1B parameters, ~4 GB FP32)\n",
    "- T5-GEMMA-2B (2B parameters, ~8 GB FP32)\n",
    "- T5-GEMMA-9B (9B parameters, ~36 GB FP32)\n",
    "\n",
    "**Model Configuration:**\n",
    "- Precision: FP32 (full precision)\n",
    "- Library: transformers (direct model access)\n",
    "- Architecture: T5-based encoder-decoder (using encoder only)\n",
    "\n",
    "**Note:** This notebook uses only the encoder portion of T5-GEMMA to extract semantic embeddings. You can easily enable/disable specific models in the model selection cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Import Dependencies\n",
    "\n",
    "This notebook requires:\n",
    "- `transformers>=4.51.0`\n",
    "- `torch>=2.0.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - pandas and numpy loaded\n",
      "  - torch 2.9.0 loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/devon7y/VS Code/LLM Factor Analysis/.conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - transformers 4.57.1 loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "print(\"  - pandas and numpy loaded\")\n",
    "\n",
    "import torch\n",
    "print(f\"  - torch {torch.__version__} loaded\")\n",
    "\n",
    "# Import transformers for T5-GEMMA\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import transformers\n",
    "print(f\"  - transformers {transformers.__version__} loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Load and Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading scale...\n",
      "Loaded 42 items\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>item</th>\n",
       "      <th>factor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S1</td>\n",
       "      <td>I found myself getting upset by quite trivial ...</td>\n",
       "      <td>Stress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A2</td>\n",
       "      <td>I was aware of dryness of my mouth.</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D3</td>\n",
       "      <td>I couldn't seem to experience any positive fee...</td>\n",
       "      <td>Depression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A4</td>\n",
       "      <td>I experienced breathing difficulty (eg, excess...</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D5</td>\n",
       "      <td>I just couldn't seem to get going.</td>\n",
       "      <td>Depression</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  code                                               item      factor\n",
       "0   S1  I found myself getting upset by quite trivial ...      Stress\n",
       "1   A2                I was aware of dryness of my mouth.     Anxiety\n",
       "2   D3  I couldn't seem to experience any positive fee...  Depression\n",
       "3   A4  I experienced breathing difficulty (eg, excess...     Anxiety\n",
       "4   D5                 I just couldn't seem to get going.  Depression"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Loading scale...\")\n",
    "scale = pd.read_csv('DASS_items.csv', usecols=['code', 'item', 'factor'])\n",
    "print(f\"Loaded {len(scale)} items\")\n",
    "\n",
    "# Preview the data\n",
    "scale.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total items: 42\n",
      "Unique factors: ['Anxiety', 'Depression', 'Stress']\n",
      "Sample codes: ['S1', 'A2', 'D3', 'A4', 'D5']\n"
     ]
    }
   ],
   "source": [
    "# Extract codes, items and factors for easier access\n",
    "codes = scale['code'].tolist()\n",
    "items = scale['item'].tolist()\n",
    "factors = scale['factor'].tolist()\n",
    "\n",
    "print(f\"Total items: {len(items)}\")\n",
    "print(f\"Unique factors: {sorted(set(factors))}\")\n",
    "print(f\"Sample codes: {codes[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Device Detection and Memory Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting available device...\n",
      "✓ Using Apple MPS GPU (Metal Performance Shaders)\n",
      "\n",
      "System Memory:\n",
      "  Total: 16.0 GB\n",
      "  Available: 5.9 GB\n",
      "  Used: 62.9%\n"
     ]
    }
   ],
   "source": [
    "# Detect available device\n",
    "print(\"Detecting available device...\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"✓ Using CUDA GPU: {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(\"✓ Using Apple MPS GPU (Metal Performance Shaders)\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"✓ Using CPU\")\n",
    "\n",
    "# Check available memory (basic check)\n",
    "try:\n",
    "    import psutil\n",
    "    mem = psutil.virtual_memory()\n",
    "    print(f\"\\nSystem Memory:\")\n",
    "    print(f\"  Total: {mem.total / (1024**3):.1f} GB\")\n",
    "    print(f\"  Available: {mem.available / (1024**3):.1f} GB\")\n",
    "    print(f\"  Used: {mem.percent}%\")\n",
    "except ImportError:\n",
    "    print(\"\\n  psutil not installed - skipping memory check\")\n",
    "    print(\"Install with: pip install psutil\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Configure and Load T5-GEMMA Encoder Models\n",
    "\n",
    "**Memory Requirements (FP32 full precision):**\n",
    "- T5-GEMMA-Base: ~4 GB\n",
    "- T5-GEMMA-2B: ~8 GB\n",
    "- T5-GEMMA-9B: ~36 GB\n",
    "\n",
    "**How to select models:**\n",
    "- In the cell below, you can easily enable/disable models by commenting/uncommenting lines\n",
    "- Simply add `#` at the start of a line to disable that model\n",
    "- Remove `#` to enable a model\n",
    "- The notebook will automatically adapt to run 1, 2, or all 3 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 1 T5-GEMMA model(s)...\n",
      "======================================================================\n",
      "Selected models: ['t5gemma-b-b-ul2']\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Loading google/t5gemma-b-b-ul2\n",
      "======================================================================\n",
      "This may take 30-180 seconds depending on model size...\n",
      "\n",
      "Loading tokenizer...\n",
      "✓ Tokenizer loaded\n",
      "\n",
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ T5-GEMMA-B model loaded successfully!\n",
      "  Device: mps\n",
      "  Model architecture: T5GemmaModel\n",
      "  Embedding dimension: 768\n",
      "\n",
      "======================================================================\n",
      "✓ Successfully loaded 1 model(s)!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MODEL SELECTION - Comment out any models you don't want to run\n",
    "# ============================================================================\n",
    "# To disable a model, add a '#' at the start of its line\n",
    "# To enable a model, remove the '#' from the start of its line\n",
    "# ============================================================================\n",
    "\n",
    "model_names = [\n",
    "    \"google/t5gemma-b-b-ul2\",      # Base model (~1B params, ~4 GB)\n",
    "    #\"google/t5gemma-2b-2b-ul2\",    # 2B model (~8 GB)\n",
    "    #\"google/t5gemma-9b-9b-ul2\",    # 9B model (~36 GB)\n",
    "]\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "# Dictionary to store results\n",
    "all_embeddings = {}\n",
    "all_models = {}\n",
    "\n",
    "print(f\"Loading {len(model_names)} T5-GEMMA model(s)...\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Selected models: {[m.split('/')[-1] for m in model_names]}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for model_name in model_names:\n",
    "    # Extract model size from name (e.g., \"b\", \"2b\", \"9b\")\n",
    "    model_size = model_name.split(\"-\")[-2].upper()  # Get \"b\", \"2b\", or \"9b\" and uppercase\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Loading {model_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"This may take 30-180 seconds depending on model size...\")\n",
    "    \n",
    "    try:\n",
    "        # Load tokenizer\n",
    "        print(\"\\nLoading tokenizer...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        print(\"✓ Tokenizer loaded\")\n",
    "        \n",
    "        # Load model (encoder-decoder architecture)\n",
    "        print(\"\\nLoading model...\")\n",
    "        model = AutoModel.from_pretrained(\n",
    "            model_name,\n",
    "            dtype=torch.float32,  # FP32 full precision\n",
    "        )\n",
    "        \n",
    "        # Move model to device\n",
    "        model = model.to(device)\n",
    "        model.eval()  # Set to evaluation mode\n",
    "        \n",
    "        print(f\"✓ T5-GEMMA-{model_size} model loaded successfully!\")\n",
    "        print(f\"  Device: {device}\")\n",
    "        print(f\"  Model architecture: {model.__class__.__name__}\")\n",
    "        \n",
    "        # Check embedding dimension by encoding a test string\n",
    "        with torch.no_grad():\n",
    "            test_inputs = tokenizer([\"test\"], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "            test_outputs = model.encoder(**test_inputs)\n",
    "            test_embedding = test_outputs.last_hidden_state[:, 0, :].cpu().numpy()  # Use first token\n",
    "        \n",
    "        print(f\"  Embedding dimension: {test_embedding.shape[1]}\")\n",
    "        \n",
    "        # Store the model and tokenizer\n",
    "        all_models[model_size] = {\"model\": model, \"tokenizer\": tokenizer}\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError loading {model_name}:\")\n",
    "        print(f\"  {type(e).__name__}: {str(e)}\")\n",
    "        print(f\"  Skipping this model and continuing with others...\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"✓ Successfully loaded {len(all_models)} model(s)!\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Extract Embeddings Using Encoder\n",
    "\n",
    "Extract embeddings for all personality items using the T5-GEMMA encoder.\n",
    "\n",
    "**Processing:**\n",
    "- Extract embeddings from encoder's last hidden state\n",
    "- Use first token (similar to [CLS] token) as sentence representation\n",
    "- Using `batch_size=8` for efficient processing\n",
    "- Results stored in `all_embeddings` dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting embeddings for 42 personality items using T5-GEMMA encoder...\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Processing with B model\n",
      "======================================================================\n",
      "  Processed batch 6/6\n",
      "\n",
      "✓ Embedding extraction complete for B!\n",
      "  Shape: (42, 768)\n",
      "  (42 items × 768 dimensions)\n",
      "\n",
      "======================================================================\n",
      "✓ All embeddings extracted successfully!\n",
      "======================================================================\n",
      "\n",
      "Embedding dimensions by model:\n",
      "  B: 768D\n"
     ]
    }
   ],
   "source": [
    "print(f\"Extracting embeddings for {len(items)} personality items using T5-GEMMA encoder...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for model_size, model_dict in all_models.items():\n",
    "    model = model_dict[\"model\"]\n",
    "    tokenizer = model_dict[\"tokenizer\"]\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Processing with {model_size} model\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    try:\n",
    "        # Extract embeddings in batches\n",
    "        batch_size = 8\n",
    "        all_item_embeddings = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(items), batch_size):\n",
    "                batch_items = items[i:i+batch_size]\n",
    "                \n",
    "                # Tokenize batch\n",
    "                inputs = tokenizer(\n",
    "                    batch_items,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    max_length=512\n",
    "                ).to(device)\n",
    "                \n",
    "                # Get encoder outputs\n",
    "                outputs = model.encoder(**inputs)\n",
    "                \n",
    "                # Extract embeddings from first token of last hidden state\n",
    "                # Shape: (batch_size, hidden_dim)\n",
    "                batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "                all_item_embeddings.append(batch_embeddings)\n",
    "                \n",
    "                print(f\"  Processed batch {i//batch_size + 1}/{(len(items)-1)//batch_size + 1}\", end=\"\\r\")\n",
    "        \n",
    "        # Concatenate all batches\n",
    "        embeddings = np.vstack(all_item_embeddings)\n",
    "        \n",
    "        # Normalize embeddings (L2 normalization)\n",
    "        embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "        \n",
    "        # Store results\n",
    "        all_embeddings[model_size] = embeddings\n",
    "        \n",
    "        print(f\"\\n\\n✓ Embedding extraction complete for {model_size}!\")\n",
    "        print(f\"  Shape: {embeddings.shape}\")\n",
    "        print(f\"  ({embeddings.shape[0]} items × {embeddings.shape[1]} dimensions)\")\n",
    "        \n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e).lower():\n",
    "            print(f\"\\nOut of memory error with {model_size}!\")\n",
    "            print(f\"  Try reducing batch_size or using CPU\")\n",
    "            raise\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"✓ All embeddings extracted successfully!\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\nEmbedding dimensions by model:\")\n",
    "for model_size, embeddings in all_embeddings.items():\n",
    "    print(f\"  {model_size}: {embeddings.shape[1]}D\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Inspect Embedding Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimensions by model:\n",
      "======================================================================\n",
      "\n",
      "B:\n",
      "  Shape: (42, 768)\n",
      "  Embedding dimension: 768D\n",
      "  First embedding (first 10 values): [ 0.04314757  0.01970899  0.01369213 -0.00909064 -0.04341049  0.01279546\n",
      " -0.00889299  0.01394581  0.00257805  0.08441301]\n"
     ]
    }
   ],
   "source": [
    "# Compare embedding dimensions across models\n",
    "print(\"Embedding dimensions by model:\")\n",
    "print(\"=\" * 70)\n",
    "for model_size, embeddings in all_embeddings.items():\n",
    "    print(f\"\\n{model_size}:\")\n",
    "    print(f\"  Shape: {embeddings.shape}\")\n",
    "    print(f\"  Embedding dimension: {embeddings.shape[1]}D\")\n",
    "    print(f\"  First embedding (first 10 values): {embeddings[0][:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding statistics by model:\n",
      "======================================================================\n",
      "\n",
      "B:\n",
      "  Min value: -0.8398\n",
      "  Max value: 0.2979\n",
      "  Mean: -0.0005\n",
      "  Std: 0.0361\n"
     ]
    }
   ],
   "source": [
    "# Summary statistics for all models\n",
    "print(\"Embedding statistics by model:\")\n",
    "print(\"=\" * 70)\n",
    "for model_size, embeddings in all_embeddings.items():\n",
    "    print(f\"\\n{model_size}:\")\n",
    "    print(f\"  Min value: {embeddings.min():.4f}\")\n",
    "    print(f\"  Max value: {embeddings.max():.4f}\")\n",
    "    print(f\"  Mean: {embeddings.mean():.4f}\")\n",
    "    print(f\"  Std: {embeddings.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample item #0:\n",
      "  Factor: Stress\n",
      "  Text: I found myself getting upset by quite trivial things.\n",
      "\n",
      "Embedding properties by model:\n",
      "======================================================================\n",
      "\n",
      "B:\n",
      "  Embedding shape: (768,)\n",
      "  Embedding norm: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Check a specific item across all models\n",
    "sample_idx = 0\n",
    "print(f\"Sample item #{sample_idx}:\")\n",
    "print(f\"  Factor: {factors[sample_idx]}\")\n",
    "print(f\"  Text: {items[sample_idx]}\")\n",
    "print(\"\\nEmbedding properties by model:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for model_size, embeddings in all_embeddings.items():\n",
    "    print(f\"\\n{model_size}:\")\n",
    "    print(f\"  Embedding shape: {embeddings[sample_idx].shape}\")\n",
    "    print(f\"  Embedding norm: {np.linalg.norm(embeddings[sample_idx]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## T-SNE Visualization\n",
    "\n",
    "Visualize the high-dimensional embeddings in 2D space using T-SNE, color-coded by personality factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization libraries loaded\n"
     ]
    }
   ],
   "source": [
    "# Import visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "print(\"Visualization libraries loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for T-SNE...\n",
      "Number of items: 42\n",
      "Personality factors: ['Anxiety', 'Depression', 'Stress']\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for T-SNE (same across all models)\n",
    "print(\"Preparing data for T-SNE...\")\n",
    "print(f\"Number of items: {len(factors)}\")\n",
    "\n",
    "# Get unique factors for legend\n",
    "unique_factors = sorted(set(factors))\n",
    "print(f\"Personality factors: {unique_factors}\")\n",
    "\n",
    "# Create a color map for the personality factors\n",
    "import matplotlib.pyplot as plt\n",
    "colors_map = plt.colormaps.get_cmap('tab10')\n",
    "factor_to_color = {factor: colors_map(i / len(unique_factors)) for i, factor in enumerate(unique_factors)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running T-SNE for all models...\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Running T-SNE for B...\n",
      "======================================================================\n",
      "Input shape: (42, 768)\n",
      "[t-SNE] Computing 41 nearest neighbors...\n",
      "[t-SNE] Indexed 42 samples in 0.000s...\n",
      "[t-SNE] Computed neighbors for 42 samples in 0.065s...\n",
      "[t-SNE] Computed conditional probabilities for sample 42 / 42\n",
      "[t-SNE] Mean sigma: 0.478738\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 46.354488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] KL divergence after 1000 iterations: 0.025370\n",
      "✓ T-SNE complete! 2D embeddings shape: (42, 2)\n",
      "\n",
      "======================================================================\n",
      "✓ T-SNE complete for all 1 models!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Run T-SNE and create visualizations for all models\n",
    "print(\"Running T-SNE for all models...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "all_tsne_embeddings = {}\n",
    "\n",
    "for model_key, embeddings in all_embeddings.items():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Running T-SNE for {model_key}...\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Input shape: {embeddings.shape}\")\n",
    "    \n",
    "    # Run T-SNE dimensionality reduction\n",
    "    tsne = TSNE(\n",
    "        n_components=2,      # Reduce to 2D\n",
    "        perplexity=30,       # Balance local vs global structure\n",
    "        max_iter=1000,       # Number of iterations\n",
    "        random_state=42,     # For reproducibility\n",
    "        verbose=1            # Show progress\n",
    "    )\n",
    "    \n",
    "    # Transform high-D embeddings to 2D\n",
    "    embeddings_2d = tsne.fit_transform(embeddings)\n",
    "    all_tsne_embeddings[model_key] = embeddings_2d\n",
    "    \n",
    "    print(f\"✓ T-SNE complete! 2D embeddings shape: {embeddings_2d.shape}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"✓ T-SNE complete for all {len(all_tsne_embeddings)} models!\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": "# Create T-SNE scatter plots for all models\nprint(\"Creating visualizations...\")\nprint(\"=\" * 70)\n\n# Create plots directory if it doesn't exist\nimport os\nfrom datetime import datetime\n\nplots_dir = \"plots\"\nos.makedirs(plots_dir, exist_ok=True)\nprint(f\"Plots will be saved to: {plots_dir}/\")\n\n# Generate timestamp for filename\ntimestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n\n# Determine number of models and create appropriate figure\nnum_models = len(all_tsne_embeddings)\nprint(f\"Creating plots for {num_models} model(s)...\")\n\n# Adjust figure size based on number of models\nfig_width = 8 * num_models  # 8 inches per model\nfig, axes = plt.subplots(1, num_models, figsize=(fig_width, 8))\n\n# Handle case of single model (axes is not a list in this case)\nif num_models == 1:\n    axes = [axes]\n\nfor idx, (model_key, embeddings_2d) in enumerate(sorted(all_tsne_embeddings.items())):\n    ax = axes[idx]\n    \n    # Plot each factor with a different color\n    for factor in unique_factors:\n        # Get indices for this factor\n        indices = [i for i, f in enumerate(factors) if f == factor]\n        \n        # Plot points for this factor\n        ax.scatter(\n            embeddings_2d[indices, 0],\n            embeddings_2d[indices, 1],\n            c=[factor_to_color[factor]],\n            label=factor,\n            alpha=0.6,\n            s=80,\n            edgecolors='white',\n            linewidth=0.5\n        )\n    \n    # Add labels for each point using the 'code' column\n    for i in range(len(embeddings_2d)):\n        ax.annotate(\n            codes[i],  # Use the code as the label\n            (embeddings_2d[i, 0], embeddings_2d[i, 1]),\n            fontsize=7,\n            alpha=0.7,\n            ha='center',\n            va='bottom',\n            xytext=(0, 3),  # Offset label slightly above the point\n            textcoords='offset points'\n        )\n    \n    # Get embedding dimension for title\n    embedding_dim = all_embeddings[model_key].shape[1]\n    \n    ax.set_xlabel('T-SNE Component 1', fontsize=11)\n    ax.set_ylabel('T-SNE Component 2', fontsize=11)\n    ax.set_title(\n        f'{model_key} Model\\n({embedding_dim}D → 2D)',\n        fontsize=13,\n        fontweight='bold'\n    )\n    ax.grid(True, alpha=0.3, linestyle='--')\n    \n    # Add legend to the rightmost plot\n    if idx == num_models - 1:\n        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n\n# Overall title\nfig.suptitle(\n    'T-SNE Visualization of DASS Item Embeddings\\n'\n    f'T5-GEMMA Encoder Models (FP32, {len(items)} items) - {num_models} model(s)',\n    fontsize=16,\n    fontweight='bold',\n    y=1.00\n)\n\nplt.tight_layout()\n\n# Save the figure\nmodel_names_str = \"_\".join(sorted(all_tsne_embeddings.keys()))\nfilename = f\"t5gemma_tsne_visualization_{model_names_str}_{timestamp}.png\"\nfilepath = os.path.join(plots_dir, filename)\nplt.savefig(filepath, dpi=300, bbox_inches='tight')\nprint(f\"\\n✓ Plot saved to: {filepath}\")\n\n# Display the plot\nplt.show()\n\nprint(\"\\n✓ Visualization complete!\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## Analyze Nearest Neighbors\n",
    "\n",
    "Compare how the encoder embeddings identify semantic neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding nearest neighbors in original embedding space (not T-SNE)...\n",
      "======================================================================\n",
      "\n",
      "Sample item #0:\n",
      "  Factor: Stress\n",
      "  Text: I found myself getting upset by quite trivial things.\n",
      "\n",
      "======================================================================\n",
      "B Model - Original 768D Space\n",
      "======================================================================\n",
      "5 Most similar items (by cosine similarity):\n",
      "  1. [Anxiety] I had a feeling of faintness.\n",
      "      Similarity: 0.9985\n",
      "  2. [Stress] I tended to over-react to situations.\n",
      "      Similarity: 0.9980\n",
      "  3. [Stress] I found myself getting agitated.\n",
      "      Similarity: 0.9979\n",
      "  4. [Depression] I felt sad and depressed.\n",
      "      Similarity: 0.9976\n",
      "  5. [Anxiety] I felt terrified.\n",
      "      Similarity: 0.9955\n"
     ]
    }
   ],
   "source": [
    "# Analyze nearest neighbors in the ORIGINAL high-dimensional space for all models\n",
    "print(\"Finding nearest neighbors in original embedding space (not T-SNE)...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(f\"\\nSample item #{sample_idx}:\")\n",
    "print(f\"  Factor: {factors[sample_idx]}\")\n",
    "print(f\"  Text: {items[sample_idx]}\")\n",
    "\n",
    "for model_size, embeddings in sorted(all_embeddings.items()):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"{model_size} Model - Original {embeddings.shape[1]}D Space\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Compute cosine similarity between sample and all items\n",
    "    similarities = cosine_similarity([embeddings[sample_idx]], embeddings)[0]\n",
    "    \n",
    "    # Find 5 most similar items (excluding itself)\n",
    "    most_similar_indices = np.argsort(similarities)[::-1][1:6]\n",
    "    \n",
    "    print(f\"5 Most similar items (by cosine similarity):\")\n",
    "    for rank, idx in enumerate(most_similar_indices, 1):\n",
    "        print(f\"  {rank}. [{factors[idx]}] {items[idx]}\")\n",
    "        print(f\"      Similarity: {similarities[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## Quantify Factor Separation\n",
    "\n",
    "Measure how well the embeddings separate the three DASS factors (Anxiety, Depression, Stress) using cosine similarity analysis.\n",
    "\n",
    "**Metrics:**\n",
    "- **Within-factor similarity**: Average cosine similarity between items in the same factor\n",
    "- **Between-factor similarity**: Average cosine similarity between items in different factors  \n",
    "- **Separation ratio**: Within / Between (higher = better separation, >1.0 means factors cluster together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cell-24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing factor separation metrics...\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "B Model - Factor Separation Analysis\n",
      "======================================================================\n",
      "\n",
      "Overall Separation Metrics:\n",
      "  Within-factor similarity:  0.5350\n",
      "  Between-factor similarity: 0.5198\n",
      "  Separation ratio:          1.0292\n",
      "    (Good separation - factors cluster together!)\n",
      "\n",
      "Per-Factor Within-Similarity:\n",
      "  Anxiety     : 0.5865 ± 0.4204  (n=91 pairs)\n",
      "  Depression  : 0.6116 ± 0.3838  (n=91 pairs)\n",
      "  Stress      : 0.4068 ± 0.5055  (n=91 pairs)\n",
      "\n",
      "Between-Factor Similarities:\n",
      "  Anxiety      vs Depression  : 0.6097 ± 0.3928  (n=196 pairs)\n",
      "  Anxiety      vs Stress      : 0.4800 ± 0.4773  (n=196 pairs)\n",
      "  Depression   vs Stress      : 0.4697 ± 0.4684  (n=196 pairs)\n",
      "\n",
      "======================================================================\n",
      "Factor separation analysis complete!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing factor separation metrics...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Loop through all models\n",
    "for model_size, embeddings in sorted(all_embeddings.items()):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"{model_size} Model - Factor Separation Analysis\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Compute full similarity matrix\n",
    "    sim_matrix = cosine_similarity(embeddings)\n",
    "    \n",
    "    # Initialize accumulators\n",
    "    within_factor_sims = {factor: [] for factor in unique_factors}\n",
    "    between_factor_sims = []\n",
    "    \n",
    "    # Compute within-factor and between-factor similarities\n",
    "    for i in range(len(items)):\n",
    "        for j in range(i + 1, len(items)):  # Only upper triangle (avoid duplicates)\n",
    "            similarity = sim_matrix[i, j]\n",
    "            \n",
    "            if factors[i] == factors[j]:\n",
    "                # Same factor - within-factor similarity\n",
    "                within_factor_sims[factors[i]].append(similarity)\n",
    "            else:\n",
    "                # Different factors - between-factor similarity\n",
    "                between_factor_sims.append(similarity)\n",
    "    \n",
    "    # Compute overall metrics\n",
    "    all_within_sims = []\n",
    "    for factor_sims in within_factor_sims.values():\n",
    "        all_within_sims.extend(factor_sims)\n",
    "    \n",
    "    within_mean = np.mean(all_within_sims)\n",
    "    between_mean = np.mean(between_factor_sims)\n",
    "    separation_ratio = within_mean / between_mean\n",
    "    \n",
    "    # Print overall results\n",
    "    print(f\"\\nOverall Separation Metrics:\")\n",
    "    print(f\"  Within-factor similarity:  {within_mean:.4f}\")\n",
    "    print(f\"  Between-factor similarity: {between_mean:.4f}\")\n",
    "    print(f\"  Separation ratio:          {separation_ratio:.4f}\")\n",
    "    print(f\"    {'(Good separation - factors cluster together!)' if separation_ratio > 1.0 else '(Poor separation - factors overlap)'}\")\n",
    "    \n",
    "    # Print per-factor breakdown\n",
    "    print(f\"\\nPer-Factor Within-Similarity:\")\n",
    "    for factor in unique_factors:\n",
    "        factor_mean = np.mean(within_factor_sims[factor])\n",
    "        factor_std = np.std(within_factor_sims[factor])\n",
    "        n_pairs = len(within_factor_sims[factor])\n",
    "        print(f\"  {factor:12s}: {factor_mean:.4f} ± {factor_std:.4f}  (n={n_pairs} pairs)\")\n",
    "    \n",
    "    # Compute pairwise between-factor similarities\n",
    "    print(f\"\\nBetween-Factor Similarities:\")\n",
    "    factor_pairs = {}\n",
    "    for i in range(len(items)):\n",
    "        for j in range(i + 1, len(items)):\n",
    "            if factors[i] != factors[j]:\n",
    "                pair = tuple(sorted([factors[i], factors[j]]))\n",
    "                if pair not in factor_pairs:\n",
    "                    factor_pairs[pair] = []\n",
    "                factor_pairs[pair].append(sim_matrix[i, j])\n",
    "    \n",
    "    for pair in sorted(factor_pairs.keys()):\n",
    "        pair_mean = np.mean(factor_pairs[pair])\n",
    "        pair_std = np.std(factor_pairs[pair])\n",
    "        n_pairs = len(factor_pairs[pair])\n",
    "        print(f\"  {pair[0]:12s} vs {pair[1]:12s}: {pair_mean:.4f} ± {pair_std:.4f}  (n={n_pairs} pairs)\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"Factor separation analysis complete!\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## Calculate Factor Centroids\n",
    "\n",
    "Compute the mean embedding (centroid) for each of the three DASS factors. These centroids represent the \"average\" embedding for each psychological dimension and can be used for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cell-26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating factor centroids...\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "B Model - Computing Centroids\n",
      "======================================================================\n",
      "\n",
      "Anxiety:\n",
      "  Number of items: 14\n",
      "  Centroid shape: (768,)\n",
      "  Centroid norm: 0.7849\n",
      "\n",
      "Depression:\n",
      "  Number of items: 14\n",
      "  Centroid shape: (768,)\n",
      "  Centroid norm: 0.7996\n",
      "\n",
      "Stress:\n",
      "  Number of items: 14\n",
      "  Centroid shape: (768,)\n",
      "  Centroid norm: 0.6702\n",
      "\n",
      "======================================================================\n",
      "Centroid calculation complete!\n",
      "======================================================================\n",
      "\n",
      "Centroids stored in 'all_centroids' dictionary:\n",
      "  Structure: all_centroids[model_size][factor] = centroid_vector\n",
      "  Models: ['B']\n",
      "  Factors per model: ['Anxiety', 'Depression', 'Stress']\n"
     ]
    }
   ],
   "source": [
    "print(\"Calculating factor centroids...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Dictionary to store centroids for all models\n",
    "all_centroids = {}\n",
    "\n",
    "# Loop through all models\n",
    "for model_size, embeddings in sorted(all_embeddings.items()):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"{model_size} Model - Computing Centroids\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Initialize centroid dictionary for this model\n",
    "    centroids = {}\n",
    "    \n",
    "    # Calculate centroid (mean embedding) for each factor\n",
    "    for factor in unique_factors:\n",
    "        # Get indices of items belonging to this factor\n",
    "        factor_indices = [i for i, f in enumerate(factors) if f == factor]\n",
    "        \n",
    "        # Get embeddings for this factor\n",
    "        factor_embeddings = embeddings[factor_indices]\n",
    "        \n",
    "        # Compute centroid (mean of all embeddings in this factor)\n",
    "        centroid = np.mean(factor_embeddings, axis=0)\n",
    "        \n",
    "        # Store centroid\n",
    "        centroids[factor] = centroid\n",
    "        \n",
    "        # Print info\n",
    "        print(f\"\\n{factor}:\")\n",
    "        print(f\"  Number of items: {len(factor_indices)}\")\n",
    "        print(f\"  Centroid shape: {centroid.shape}\")\n",
    "        print(f\"  Centroid norm: {np.linalg.norm(centroid):.4f}\")\n",
    "    \n",
    "    # Store centroids for this model\n",
    "    all_centroids[model_size] = centroids\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"Centroid calculation complete!\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\nCentroids stored in 'all_centroids' dictionary:\")\n",
    "print(f\"  Structure: all_centroids[model_size][factor] = centroid_vector\")\n",
    "print(f\"  Models: {list(all_centroids.keys())}\")\n",
    "print(f\"  Factors per model: {list(all_centroids[list(all_centroids.keys())[0]].keys())}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}