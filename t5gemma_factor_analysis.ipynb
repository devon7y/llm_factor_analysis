{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# LLM Factor Analysis - Personality Items (T5-GEMMA Encoder)\n",
    "\n",
    "Extracts embeddings from DASS personality items using **T5-GEMMA encoder** and compares predicted similarities with observed correlations.\n",
    "\n",
    "**Available Models:**\n",
    "- T5-GEMMA-Base (~1B parameters, ~4 GB FP32)\n",
    "- T5-GEMMA-2B (2B parameters, ~8 GB FP32)\n",
    "- T5-GEMMA-9B (9B parameters, ~36 GB FP32)\n",
    "\n",
    "**Model Configuration:**\n",
    "- Precision: FP32 (full precision)\n",
    "- Library: transformers (direct model access)\n",
    "- Architecture: T5-based encoder-decoder (using encoder only)\n",
    "\n",
    "**Note:** This notebook uses only the encoder portion of T5-GEMMA to extract semantic embeddings. You can easily enable/disable specific models in the model selection cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Import Dependencies\n",
    "\n",
    "This notebook requires:\n",
    "- `transformers>=4.51.0`\n",
    "- `torch>=2.0.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - pandas and numpy loaded\n",
      "  - torch 2.9.0 loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/devon7y/VS Code/LLM Factor Analysis/.conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - transformers 4.57.1 loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "print(\"  - pandas and numpy loaded\")\n",
    "\n",
    "import torch\n",
    "print(f\"  - torch {torch.__version__} loaded\")\n",
    "\n",
    "# Import transformers for T5-GEMMA\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import transformers\n",
    "print(f\"  - transformers {transformers.__version__} loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Load and Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading scale...\n",
      "Loaded 30 items\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>item</th>\n",
       "      <th>factor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B1</td>\n",
       "      <td>I avoid tasks that I know I need to do.</td>\n",
       "      <td>Behavioral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B2</td>\n",
       "      <td>I postpone tasks until the last minute.</td>\n",
       "      <td>Behavioral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B3</td>\n",
       "      <td>I find myself saying 'I'll do it tomorrow.'</td>\n",
       "      <td>Behavioral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B4</td>\n",
       "      <td>I have difficulty managing my time for assignm...</td>\n",
       "      <td>Behavioral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B5</td>\n",
       "      <td>I find that tasks take longer than expected.</td>\n",
       "      <td>Behavioral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  code                                               item      factor\n",
       "0   B1            I avoid tasks that I know I need to do.  Behavioral\n",
       "1   B2            I postpone tasks until the last minute.  Behavioral\n",
       "2   B3        I find myself saying 'I'll do it tomorrow.'  Behavioral\n",
       "3   B4  I have difficulty managing my time for assignm...  Behavioral\n",
       "4   B5       I find that tasks take longer than expected.  Behavioral"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Loading scale...\")\n",
    "scale = pd.read_csv('procrastination_items.csv', usecols=['code', 'item', 'factor'])\n",
    "print(f\"Loaded {len(scale)} items\")\n",
    "\n",
    "# Preview the data\n",
    "scale.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total items: 30\n",
      "Unique factors: ['Behavioral', 'Emotional']\n",
      "Sample codes: ['B1', 'B2', 'B3', 'B4', 'B5']\n"
     ]
    }
   ],
   "source": [
    "# Extract codes, items and factors for easier access\n",
    "codes = scale['code'].tolist()\n",
    "items = scale['item'].tolist()\n",
    "factors = scale['factor'].tolist()\n",
    "\n",
    "print(f\"Total items: {len(items)}\")\n",
    "print(f\"Unique factors: {sorted(set(factors))}\")\n",
    "print(f\"Sample codes: {codes[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Device Detection and Memory Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting available device...\n",
      "✓ Using Apple MPS GPU (Metal Performance Shaders)\n",
      "\n",
      "System Memory:\n",
      "  Total: 16.0 GB\n",
      "  Available: 2.9 GB\n",
      "  Used: 82.1%\n"
     ]
    }
   ],
   "source": [
    "# Detect available device\n",
    "print(\"Detecting available device...\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"✓ Using CUDA GPU: {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(\"✓ Using Apple MPS GPU (Metal Performance Shaders)\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"✓ Using CPU\")\n",
    "\n",
    "# Check available memory (basic check)\n",
    "try:\n",
    "    import psutil\n",
    "    mem = psutil.virtual_memory()\n",
    "    print(f\"\\nSystem Memory:\")\n",
    "    print(f\"  Total: {mem.total / (1024**3):.1f} GB\")\n",
    "    print(f\"  Available: {mem.available / (1024**3):.1f} GB\")\n",
    "    print(f\"  Used: {mem.percent}%\")\n",
    "except ImportError:\n",
    "    print(\"\\n  psutil not installed - skipping memory check\")\n",
    "    print(\"Install with: pip install psutil\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Configure and Load T5-GEMMA Encoder Models\n",
    "\n",
    "**Memory Requirements (FP32 full precision):**\n",
    "- T5-GEMMA-Base: ~4 GB\n",
    "- T5-GEMMA-2B: ~8 GB\n",
    "- T5-GEMMA-9B: ~36 GB\n",
    "\n",
    "**How to select models:**\n",
    "- In the cell below, you can easily enable/disable models by commenting/uncommenting lines\n",
    "- Simply add `#` at the start of a line to disable that model\n",
    "- Remove `#` to enable a model\n",
    "- The notebook will automatically adapt to run 1, 2, or all 3 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 1 T5-GEMMA model(s)...\n",
      "======================================================================\n",
      "Selected models: ['t5gemma-b-b-ul2']\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Loading google/t5gemma-b-b-ul2\n",
      "======================================================================\n",
      "This may take 30-180 seconds depending on model size...\n",
      "\n",
      "Loading tokenizer...\n",
      "✓ Tokenizer loaded\n",
      "\n",
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ T5-GEMMA-B model loaded successfully!\n",
      "  Device: mps\n",
      "  Model architecture: T5GemmaModel\n",
      "  Embedding dimension: 768\n",
      "\n",
      "======================================================================\n",
      "✓ Successfully loaded 1 model(s)!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MODEL SELECTION - Comment out any models you don't want to run\n",
    "# ============================================================================\n",
    "# To disable a model, add a '#' at the start of its line\n",
    "# To enable a model, remove the '#' from the start of its line\n",
    "# ============================================================================\n",
    "\n",
    "model_names = [\n",
    "    \"google/t5gemma-b-b-ul2\",      # Base model (~1B params, ~4 GB)\n",
    "    #\"google/t5gemma-2b-2b-ul2\",    # 2B model (~8 GB)\n",
    "    #\"google/t5gemma-9b-9b-ul2\",    # 9B model (~36 GB)\n",
    "]\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "# Dictionary to store results\n",
    "all_embeddings = {}\n",
    "all_models = {}\n",
    "\n",
    "print(f\"Loading {len(model_names)} T5-GEMMA model(s)...\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Selected models: {[m.split('/')[-1] for m in model_names]}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for model_name in model_names:\n",
    "    # Extract model size from name (e.g., \"b\", \"2b\", \"9b\")\n",
    "    model_size = model_name.split(\"-\")[-2].upper()  # Get \"b\", \"2b\", or \"9b\" and uppercase\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Loading {model_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"This may take 30-180 seconds depending on model size...\")\n",
    "    \n",
    "    try:\n",
    "        # Load tokenizer\n",
    "        print(\"\\nLoading tokenizer...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        print(\"✓ Tokenizer loaded\")\n",
    "        \n",
    "        # Load model (encoder-decoder architecture)\n",
    "        print(\"\\nLoading model...\")\n",
    "        model = AutoModel.from_pretrained(\n",
    "            model_name,\n",
    "            dtype=torch.float32,  # FP32 full precision\n",
    "        )\n",
    "        \n",
    "        # Move model to device\n",
    "        model = model.to(device)\n",
    "        model.eval()  # Set to evaluation mode\n",
    "        \n",
    "        print(f\"✓ T5-GEMMA-{model_size} model loaded successfully!\")\n",
    "        print(f\"  Device: {device}\")\n",
    "        print(f\"  Model architecture: {model.__class__.__name__}\")\n",
    "        \n",
    "        # Check embedding dimension by encoding a test string\n",
    "        with torch.no_grad():\n",
    "            test_inputs = tokenizer([\"test\"], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "            test_outputs = model.encoder(**test_inputs)\n",
    "            test_embedding = test_outputs.last_hidden_state[:, 0, :].cpu().numpy()  # Use first token\n",
    "        \n",
    "        print(f\"  Embedding dimension: {test_embedding.shape[1]}\")\n",
    "        \n",
    "        # Store the model and tokenizer\n",
    "        all_models[model_size] = {\"model\": model, \"tokenizer\": tokenizer}\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError loading {model_name}:\")\n",
    "        print(f\"  {type(e).__name__}: {str(e)}\")\n",
    "        print(f\"  Skipping this model and continuing with others...\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"✓ Successfully loaded {len(all_models)} model(s)!\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Extract Embeddings Using Encoder\n",
    "\n",
    "Extract embeddings for all personality items using the T5-GEMMA encoder.\n",
    "\n",
    "**Processing:**\n",
    "- Extract embeddings from encoder's last hidden state using **mean pooling**\n",
    "- T5 models don't have a [CLS] token, so we average across all tokens (accounting for padding)\n",
    "- Using `batch_size=8` for efficient processing\n",
    "- Results stored in `all_embeddings` dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting embeddings for 30 personality items using T5-GEMMA encoder...\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Processing with B model\n",
      "======================================================================\n",
      "  Processed batch 4/4\n",
      "\n",
      "✓ Embedding extraction complete for B!\n",
      "  Shape: (30, 768)\n",
      "  (30 items × 768 dimensions)\n",
      "\n",
      "======================================================================\n",
      "✓ All embeddings extracted successfully!\n",
      "======================================================================\n",
      "\n",
      "Embedding dimensions by model:\n",
      "  B: 768D\n"
     ]
    }
   ],
   "source": [
    "print(f\"Extracting embeddings for {len(items)} personality items using T5-GEMMA encoder...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for model_size, model_dict in all_models.items():\n",
    "    model = model_dict[\"model\"]\n",
    "    tokenizer = model_dict[\"tokenizer\"]\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Processing with {model_size} model\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    try:\n",
    "        # Extract embeddings in batches\n",
    "        batch_size = 8\n",
    "        all_item_embeddings = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(items), batch_size):\n",
    "                batch_items = items[i:i+batch_size]\n",
    "                \n",
    "                # Tokenize batch\n",
    "                inputs = tokenizer(\n",
    "                    batch_items,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    max_length=512\n",
    "                ).to(device)\n",
    "                \n",
    "                # Get encoder outputs\n",
    "                outputs = model.encoder(**inputs)\n",
    "                \n",
    "                # Mean pooling with attention mask (T5 doesn't have [CLS] token)\n",
    "                # This properly accounts for padding tokens\n",
    "                attention_mask = inputs['attention_mask']\n",
    "                # Expand mask to match hidden state dimensions: (batch_size, seq_len, hidden_dim)\n",
    "                mask_expanded = attention_mask.unsqueeze(-1).expand(outputs.last_hidden_state.size()).float()\n",
    "                # Sum embeddings (masked) across sequence dimension\n",
    "                sum_embeddings = torch.sum(outputs.last_hidden_state * mask_expanded, 1)\n",
    "                # Sum mask to get actual token counts (avoid division by zero)\n",
    "                sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)\n",
    "                # Mean pooling: divide sum by count\n",
    "                batch_embeddings = (sum_embeddings / sum_mask).cpu().numpy()\n",
    "                \n",
    "                all_item_embeddings.append(batch_embeddings)\n",
    "                \n",
    "                print(f\"  Processed batch {i//batch_size + 1}/{(len(items)-1)//batch_size + 1}\", end=\"\\r\")\n",
    "        \n",
    "        # Concatenate all batches\n",
    "        embeddings = np.vstack(all_item_embeddings)\n",
    "        \n",
    "        # Normalize embeddings (L2 normalization)\n",
    "        embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "        \n",
    "        # Store results\n",
    "        all_embeddings[model_size] = embeddings\n",
    "        \n",
    "        print(f\"\\n\\n✓ Embedding extraction complete for {model_size}!\")\n",
    "        print(f\"  Shape: {embeddings.shape}\")\n",
    "        print(f\"  ({embeddings.shape[0]} items × {embeddings.shape[1]} dimensions)\")\n",
    "        \n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e).lower():\n",
    "            print(f\"\\nOut of memory error with {model_size}!\")\n",
    "            print(f\"  Try reducing batch_size or using CPU\")\n",
    "            raise\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"✓ All embeddings extracted successfully!\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\nEmbedding dimensions by model:\")\n",
    "for model_size, embeddings in all_embeddings.items():\n",
    "    print(f\"  {model_size}: {embeddings.shape[1]}D\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Inspect Embedding Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimensions by model:\n",
      "======================================================================\n",
      "\n",
      "B:\n",
      "  Shape: (30, 768)\n",
      "  Embedding dimension: 768D\n",
      "  First embedding (first 10 values): [ 0.02409234 -0.05277688  0.00503304 -0.03316708  0.02312888 -0.01533958\n",
      " -0.02701074 -0.01654503 -0.08448198 -0.00379935]\n"
     ]
    }
   ],
   "source": [
    "# Compare embedding dimensions across models\n",
    "print(\"Embedding dimensions by model:\")\n",
    "print(\"=\" * 70)\n",
    "for model_size, embeddings in all_embeddings.items():\n",
    "    print(f\"\\n{model_size}:\")\n",
    "    print(f\"  Shape: {embeddings.shape}\")\n",
    "    print(f\"  Embedding dimension: {embeddings.shape[1]}D\")\n",
    "    print(f\"  First embedding (first 10 values): {embeddings[0][:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding statistics by model:\n",
      "======================================================================\n",
      "\n",
      "B:\n",
      "  Min value: -0.3519\n",
      "  Max value: 0.2400\n",
      "  Mean: -0.0012\n",
      "  Std: 0.0361\n"
     ]
    }
   ],
   "source": [
    "# Summary statistics for all models\n",
    "print(\"Embedding statistics by model:\")\n",
    "print(\"=\" * 70)\n",
    "for model_size, embeddings in all_embeddings.items():\n",
    "    print(f\"\\n{model_size}:\")\n",
    "    print(f\"  Min value: {embeddings.min():.4f}\")\n",
    "    print(f\"  Max value: {embeddings.max():.4f}\")\n",
    "    print(f\"  Mean: {embeddings.mean():.4f}\")\n",
    "    print(f\"  Std: {embeddings.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample item #0:\n",
      "  Factor: Behavioral\n",
      "  Text: I avoid tasks that I know I need to do.\n",
      "\n",
      "Embedding properties by model:\n",
      "======================================================================\n",
      "\n",
      "B:\n",
      "  Embedding shape: (768,)\n",
      "  Embedding norm: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Check a specific item across all models\n",
    "sample_idx = 0\n",
    "print(f\"Sample item #{sample_idx}:\")\n",
    "print(f\"  Factor: {factors[sample_idx]}\")\n",
    "print(f\"  Text: {items[sample_idx]}\")\n",
    "print(\"\\nEmbedding properties by model:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for model_size, embeddings in all_embeddings.items():\n",
    "    print(f\"\\n{model_size}:\")\n",
    "    print(f\"  Embedding shape: {embeddings[sample_idx].shape}\")\n",
    "    print(f\"  Embedding norm: {np.linalg.norm(embeddings[sample_idx]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## T-SNE Visualization\n",
    "\n",
    "Visualize the high-dimensional embeddings in 2D space using T-SNE, color-coded by personality factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization libraries loaded\n"
     ]
    }
   ],
   "source": [
    "# Import visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "print(\"Visualization libraries loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for T-SNE...\n",
      "Number of items: 30\n",
      "Personality factors: ['Behavioral', 'Emotional']\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for T-SNE (same across all models)\n",
    "print(\"Preparing data for T-SNE...\")\n",
    "print(f\"Number of items: {len(factors)}\")\n",
    "\n",
    "# Get unique factors for legend\n",
    "unique_factors = sorted(set(factors))\n",
    "print(f\"Personality factors: {unique_factors}\")\n",
    "\n",
    "# Create a color map for the personality factors\n",
    "import matplotlib.pyplot as plt\n",
    "colors_map = plt.colormaps.get_cmap('tab10')\n",
    "factor_to_color = {factor: colors_map(i / len(unique_factors)) for i, factor in enumerate(unique_factors)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running T-SNE for all models...\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Running T-SNE for B...\n",
      "======================================================================\n",
      "Input shape: (30, 768)\n",
      "[t-SNE] Computing 29 nearest neighbors...\n",
      "[t-SNE] Indexed 30 samples in 0.000s...\n",
      "[t-SNE] Computed neighbors for 30 samples in 0.048s...\n",
      "[t-SNE] Computed conditional probabilities for sample 30 / 30\n",
      "[t-SNE] Mean sigma: 0.361784\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 40.709503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] KL divergence after 1000 iterations: 0.040229\n",
      "✓ T-SNE complete! 2D embeddings shape: (30, 2)\n",
      "\n",
      "======================================================================\n",
      "✓ T-SNE complete for all 1 models!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Run T-SNE and create visualizations for all models\n",
    "print(\"Running T-SNE for all models...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "all_tsne_embeddings = {}\n",
    "\n",
    "for model_key, embeddings in all_embeddings.items():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Running T-SNE for {model_key}...\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Input shape: {embeddings.shape}\")\n",
    "    \n",
    "    # Run T-SNE dimensionality reduction\n",
    "    tsne = TSNE(\n",
    "        n_components=2,      # Reduce to 2D\n",
    "        perplexity=25,       # Balance local vs global structure\n",
    "        max_iter=1000,       # Number of iterations\n",
    "        random_state=42,     # For reproducibility\n",
    "        verbose=1            # Show progress\n",
    "    )\n",
    "    \n",
    "    # Transform high-D embeddings to 2D\n",
    "    embeddings_2d = tsne.fit_transform(embeddings)\n",
    "    all_tsne_embeddings[model_key] = embeddings_2d\n",
    "    \n",
    "    print(f\"✓ T-SNE complete! 2D embeddings shape: {embeddings_2d.shape}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"✓ T-SNE complete for all {len(all_tsne_embeddings)} models!\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create T-SNE scatter plots for all models\n",
    "print(\"Creating visualizations...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create plots directory if it doesn't exist\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "plots_dir = \"plots\"\n",
    "os.makedirs(plots_dir, exist_ok=True)\n",
    "print(f\"Plots will be saved to: {plots_dir}/\")\n",
    "\n",
    "# Generate timestamp for filename\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Determine number of models and create appropriate figure\n",
    "num_models = len(all_tsne_embeddings)\n",
    "print(f\"Creating plots for {num_models} model(s)...\")\n",
    "\n",
    "# Check if we have any models to plot\n",
    "if num_models == 0:\n",
    "    print(\"\\n⚠ WARNING: No T-SNE embeddings available to plot!\")\n",
    "    print(\"  This could mean:\")\n",
    "    print(\"  - No models were successfully loaded\")\n",
    "    print(\"  - Embedding extraction failed for all models\")\n",
    "    print(\"  - T-SNE failed for all models\")\n",
    "    print(\"\\nSkipping visualization...\")\n",
    "else:\n",
    "    # Adjust figure size based on number of models\n",
    "    fig_width = 8 * num_models  # 8 inches per model\n",
    "    fig, axes = plt.subplots(1, num_models, figsize=(fig_width, 8))\n",
    "    \n",
    "    # Handle case of single model (axes is not a list in this case)\n",
    "    if num_models == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, (model_key, embeddings_2d) in enumerate(sorted(all_tsne_embeddings.items())):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Plot each factor with a different color\n",
    "        for factor in unique_factors:\n",
    "            # Get indices for this factor\n",
    "            indices = [i for i, f in enumerate(factors) if f == factor]\n",
    "            \n",
    "            # Plot points for this factor\n",
    "            ax.scatter(\n",
    "                embeddings_2d[indices, 0],\n",
    "                embeddings_2d[indices, 1],\n",
    "                c=[factor_to_color[factor]],\n",
    "                label=factor,\n",
    "                alpha=0.6,\n",
    "                s=80,\n",
    "                edgecolors='white',\n",
    "                linewidth=0.5\n",
    "            )\n",
    "        \n",
    "        # Add labels for each point using the 'code' column\n",
    "        for i in range(len(embeddings_2d)):\n",
    "            ax.annotate(\n",
    "                codes[i],  # Use the code as the label\n",
    "                (embeddings_2d[i, 0], embeddings_2d[i, 1]),\n",
    "                fontsize=7,\n",
    "                alpha=0.7,\n",
    "                ha='center',\n",
    "                va='bottom',\n",
    "                xytext=(0, 3),  # Offset label slightly above the point\n",
    "                textcoords='offset points'\n",
    "            )\n",
    "        \n",
    "        # Get embedding dimension for title\n",
    "        embedding_dim = all_embeddings[model_key].shape[1]\n",
    "        \n",
    "        ax.set_xlabel('T-SNE Component 1', fontsize=11)\n",
    "        ax.set_ylabel('T-SNE Component 2', fontsize=11)\n",
    "        ax.set_title(\n",
    "            f'{model_key} Model\\n({embedding_dim}D → 2D)',\n",
    "            fontsize=13,\n",
    "            fontweight='bold'\n",
    "        )\n",
    "        ax.grid(True, alpha=0.3, linestyle='--')\n",
    "        \n",
    "        # Add legend to the rightmost plot\n",
    "        if idx == num_models - 1:\n",
    "            ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
    "    \n",
    "    # Overall title\n",
    "    fig.suptitle(\n",
    "        'T-SNE Visualization of DASS Item Embeddings\\n'\n",
    "        f'T5-GEMMA Encoder Models (FP32, {len(items)} items) - {num_models} model(s)',\n",
    "        fontsize=16,\n",
    "        fontweight='bold',\n",
    "        y=1.00\n",
    "    )\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure\n",
    "    model_names_str = \"_\".join(sorted(all_tsne_embeddings.keys()))\n",
    "    filename = f\"t5gemma_tsne_visualization_{model_names_str}_{timestamp}.png\"\n",
    "    filepath = os.path.join(plots_dir, filename)\n",
    "    plt.savefig(filepath, dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\n✓ Plot saved to: {filepath}\")\n",
    "    \n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n✓ Visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## Analyze Nearest Neighbors\n",
    "\n",
    "Compare how the encoder embeddings identify semantic neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding nearest neighbors in original embedding space (not T-SNE)...\n",
      "======================================================================\n",
      "\n",
      "Sample item #0:\n",
      "  Factor: Behavioral\n",
      "  Text: I avoid tasks that I know I need to do.\n",
      "\n",
      "======================================================================\n",
      "B Model - Original 768D Space\n",
      "======================================================================\n",
      "5 Most similar items (by cosine similarity):\n",
      "  1. [Emotional] I lose sleep because I am anxious about unfinished tasks that I should have done.\n",
      "      Similarity: 0.8569\n",
      "  2. [Behavioral] I put off starting tasks when they are important.\n",
      "      Similarity: 0.8542\n",
      "  3. [Behavioral] I delay important tasks even though I know it will create unnecessary stress later.\n",
      "      Similarity: 0.8355\n",
      "  4. [Behavioral] I find myself engaging in unrelated activities when I should be focusing on my main task.\n",
      "      Similarity: 0.8337\n",
      "  5. [Behavioral] I spend time on unimportant tasks when I have more urgent tasks to complete.\n",
      "      Similarity: 0.8268\n"
     ]
    }
   ],
   "source": [
    "# Analyze nearest neighbors in the ORIGINAL high-dimensional space for all models\n",
    "print(\"Finding nearest neighbors in original embedding space (not T-SNE)...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(f\"\\nSample item #{sample_idx}:\")\n",
    "print(f\"  Factor: {factors[sample_idx]}\")\n",
    "print(f\"  Text: {items[sample_idx]}\")\n",
    "\n",
    "for model_size, embeddings in sorted(all_embeddings.items()):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"{model_size} Model - Original {embeddings.shape[1]}D Space\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Compute cosine similarity between sample and all items\n",
    "    similarities = cosine_similarity([embeddings[sample_idx]], embeddings)[0]\n",
    "    \n",
    "    # Find 5 most similar items (excluding itself)\n",
    "    most_similar_indices = np.argsort(similarities)[::-1][1:6]\n",
    "    \n",
    "    print(f\"5 Most similar items (by cosine similarity):\")\n",
    "    for rank, idx in enumerate(most_similar_indices, 1):\n",
    "        print(f\"  {rank}. [{factors[idx]}] {items[idx]}\")\n",
    "        print(f\"      Similarity: {similarities[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## Quantify Factor Separation\n",
    "\n",
    "Measure how well the embeddings separate the three DASS factors (Anxiety, Depression, Stress) using cosine similarity analysis.\n",
    "\n",
    "**Metrics:**\n",
    "- **Within-factor similarity**: Average cosine similarity between items in the same factor\n",
    "- **Between-factor similarity**: Average cosine similarity between items in different factors  \n",
    "- **Separation ratio**: Within / Between (higher = better separation, >1.0 means factors cluster together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cell-24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing factor separation metrics...\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "B Model - Factor Separation Analysis\n",
      "======================================================================\n",
      "\n",
      "Overall Separation Metrics:\n",
      "  Within-factor similarity:  0.8181\n",
      "  Between-factor similarity: 0.8027\n",
      "  Separation ratio:          1.0192\n",
      "    (Good separation - factors cluster together!)\n",
      "\n",
      "Per-Factor Within-Similarity:\n",
      "  Behavioral  : 0.8000 ± 0.0525  (n=120 pairs)\n",
      "  Emotional   : 0.8420 ± 0.0317  (n=91 pairs)\n",
      "\n",
      "Between-Factor Similarities:\n",
      "  Behavioral   vs Emotional   : 0.8027 ± 0.0505  (n=224 pairs)\n",
      "\n",
      "======================================================================\n",
      "Factor separation analysis complete!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing factor separation metrics...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Loop through all models\n",
    "for model_size, embeddings in sorted(all_embeddings.items()):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"{model_size} Model - Factor Separation Analysis\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Compute full similarity matrix\n",
    "    sim_matrix = cosine_similarity(embeddings)\n",
    "    \n",
    "    # Initialize accumulators\n",
    "    within_factor_sims = {factor: [] for factor in unique_factors}\n",
    "    between_factor_sims = []\n",
    "    \n",
    "    # Compute within-factor and between-factor similarities\n",
    "    for i in range(len(items)):\n",
    "        for j in range(i + 1, len(items)):  # Only upper triangle (avoid duplicates)\n",
    "            similarity = sim_matrix[i, j]\n",
    "            \n",
    "            if factors[i] == factors[j]:\n",
    "                # Same factor - within-factor similarity\n",
    "                within_factor_sims[factors[i]].append(similarity)\n",
    "            else:\n",
    "                # Different factors - between-factor similarity\n",
    "                between_factor_sims.append(similarity)\n",
    "    \n",
    "    # Compute overall metrics\n",
    "    all_within_sims = []\n",
    "    for factor_sims in within_factor_sims.values():\n",
    "        all_within_sims.extend(factor_sims)\n",
    "    \n",
    "    within_mean = np.mean(all_within_sims)\n",
    "    between_mean = np.mean(between_factor_sims)\n",
    "    separation_ratio = within_mean / between_mean\n",
    "    \n",
    "    # Print overall results\n",
    "    print(f\"\\nOverall Separation Metrics:\")\n",
    "    print(f\"  Within-factor similarity:  {within_mean:.4f}\")\n",
    "    print(f\"  Between-factor similarity: {between_mean:.4f}\")\n",
    "    print(f\"  Separation ratio:          {separation_ratio:.4f}\")\n",
    "    print(f\"    {'(Good separation - factors cluster together!)' if separation_ratio > 1.0 else '(Poor separation - factors overlap)'}\")\n",
    "    \n",
    "    # Print per-factor breakdown\n",
    "    print(f\"\\nPer-Factor Within-Similarity:\")\n",
    "    for factor in unique_factors:\n",
    "        factor_mean = np.mean(within_factor_sims[factor])\n",
    "        factor_std = np.std(within_factor_sims[factor])\n",
    "        n_pairs = len(within_factor_sims[factor])\n",
    "        print(f\"  {factor:12s}: {factor_mean:.4f} ± {factor_std:.4f}  (n={n_pairs} pairs)\")\n",
    "    \n",
    "    # Compute pairwise between-factor similarities\n",
    "    print(f\"\\nBetween-Factor Similarities:\")\n",
    "    factor_pairs = {}\n",
    "    for i in range(len(items)):\n",
    "        for j in range(i + 1, len(items)):\n",
    "            if factors[i] != factors[j]:\n",
    "                pair = tuple(sorted([factors[i], factors[j]]))\n",
    "                if pair not in factor_pairs:\n",
    "                    factor_pairs[pair] = []\n",
    "                factor_pairs[pair].append(sim_matrix[i, j])\n",
    "    \n",
    "    for pair in sorted(factor_pairs.keys()):\n",
    "        pair_mean = np.mean(factor_pairs[pair])\n",
    "        pair_std = np.std(factor_pairs[pair])\n",
    "        n_pairs = len(factor_pairs[pair])\n",
    "        print(f\"  {pair[0]:12s} vs {pair[1]:12s}: {pair_mean:.4f} ± {pair_std:.4f}  (n={n_pairs} pairs)\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"Factor separation analysis complete!\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## Calculate Factor Centroids\n",
    "\n",
    "Compute the mean embedding (centroid) for each of the three DASS factors. These centroids represent the \"average\" embedding for each psychological dimension and can be used for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cell-26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating factor centroids...\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "B Model - Computing Centroids\n",
      "======================================================================\n",
      "\n",
      "Behavioral:\n",
      "  Number of items: 16\n",
      "  Centroid shape: (768,)\n",
      "  Centroid norm: 0.9014\n",
      "\n",
      "Emotional:\n",
      "  Number of items: 14\n",
      "  Centroid shape: (768,)\n",
      "  Centroid norm: 0.9237\n",
      "\n",
      "======================================================================\n",
      "Centroid calculation complete!\n",
      "======================================================================\n",
      "\n",
      "Centroids stored in 'all_centroids' dictionary:\n",
      "  Structure: all_centroids[model_size][factor] = centroid_vector\n",
      "  Models: ['B']\n",
      "  Factors per model: ['Behavioral', 'Emotional']\n"
     ]
    }
   ],
   "source": [
    "print(\"Calculating factor centroids...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Dictionary to store centroids for all models\n",
    "all_centroids = {}\n",
    "\n",
    "# Loop through all models\n",
    "for model_size, embeddings in sorted(all_embeddings.items()):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"{model_size} Model - Computing Centroids\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Initialize centroid dictionary for this model\n",
    "    centroids = {}\n",
    "    \n",
    "    # Calculate centroid (mean embedding) for each factor\n",
    "    for factor in unique_factors:\n",
    "        # Get indices of items belonging to this factor\n",
    "        factor_indices = [i for i, f in enumerate(factors) if f == factor]\n",
    "        \n",
    "        # Get embeddings for this factor\n",
    "        factor_embeddings = embeddings[factor_indices]\n",
    "        \n",
    "        # Compute centroid (mean of all embeddings in this factor)\n",
    "        centroid = np.mean(factor_embeddings, axis=0)\n",
    "        \n",
    "        # Store centroid\n",
    "        centroids[factor] = centroid\n",
    "        \n",
    "        # Print info\n",
    "        print(f\"\\n{factor}:\")\n",
    "        print(f\"  Number of items: {len(factor_indices)}\")\n",
    "        print(f\"  Centroid shape: {centroid.shape}\")\n",
    "        print(f\"  Centroid norm: {np.linalg.norm(centroid):.4f}\")\n",
    "    \n",
    "    # Store centroids for this model\n",
    "    all_centroids[model_size] = centroids\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"Centroid calculation complete!\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\nCentroids stored in 'all_centroids' dictionary:\")\n",
    "print(f\"  Structure: all_centroids[model_size][factor] = centroid_vector\")\n",
    "print(f\"  Models: {list(all_centroids.keys())}\")\n",
    "print(f\"  Factors per model: {list(all_centroids[list(all_centroids.keys())[0]].keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pypil6vgx1",
   "metadata": {},
   "source": [
    "## Decode Factor Centroids into Labels\n",
    "\n",
    "Now we'll use the T5-GEMMA decoder to generate text labels for each factor centroid.\n",
    "\n",
    "**Approach:**\n",
    "- Load the full encoder-decoder models (with language modeling heads)\n",
    "- Encode a task prompt: \"describe this factor:\"\n",
    "- Append each factor centroid as an additional encoder token\n",
    "- Use the decoder to generate a short descriptive label\n",
    "\n",
    "This approach treats the centroid as a semantic representation that the decoder can \"read\" and describe in natural language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "okzo85gt2q",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading encoder-decoder models for centroid decoding...\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Loading Seq2Seq model: google/t5gemma-b-b-prefixlm-it\n",
      "======================================================================\n",
      "This may take 30-180 seconds depending on model size...\n",
      "\n",
      "Loading encoder-decoder model with LM head...\n",
      "✓ T5-GEMMA-PREFIXLM Seq2Seq model loaded successfully!\n",
      "  Device: mps\n",
      "  Model architecture: T5GemmaForConditionalGeneration\n",
      "\n",
      "======================================================================\n",
      "✓ Successfully loaded 1 Seq2Seq model(s)!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Import Seq2Seq model class for text generation\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "print(\"Loading encoder-decoder models for centroid decoding...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Dictionary to store decoder models\n",
    "all_decoder_models = {}\n",
    "\n",
    "# Load Seq2Seq models for all enabled model sizes\n",
    "for model_name in model_names:\n",
    "    # Extract model size from name (e.g., \"b\", \"2b\", \"9b\")\n",
    "    model_size = model_name.split(\"-\")[-2].upper()  # Get \"b\", \"2b\", or \"9b\" and uppercase\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Loading Seq2Seq model: {model_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"This may take 30-180 seconds depending on model size...\")\n",
    "    \n",
    "    try:\n",
    "        # Load full encoder-decoder model with LM head\n",
    "        print(\"\\nLoading encoder-decoder model with LM head...\")\n",
    "        decoder_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float32,  # FP32 full precision\n",
    "        )\n",
    "        \n",
    "        # Move model to device\n",
    "        decoder_model = decoder_model.to(device)\n",
    "        decoder_model.eval()  # Set to evaluation mode\n",
    "        \n",
    "        print(f\"✓ T5-GEMMA-{model_size} Seq2Seq model loaded successfully!\")\n",
    "        print(f\"  Device: {device}\")\n",
    "        print(f\"  Model architecture: {decoder_model.__class__.__name__}\")\n",
    "        \n",
    "        # Store the decoder model (tokenizer already loaded in all_models)\n",
    "        all_decoder_models[model_size] = decoder_model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError loading {model_name}:\")\n",
    "        print(f\"  {type(e).__name__}: {str(e)}\")\n",
    "        print(f\"  Skipping this model and continuing with others...\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"✓ Successfully loaded {len(all_decoder_models)} Seq2Seq model(s)!\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3pi8qz3x6bz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up redundant encoder-only models to free memory\n",
    "import gc\n",
    "\n",
    "print(\"Removing encoder-only models (Seq2Seq models contain encoders)...\")\n",
    "\n",
    "# Delete encoder-only models from all_models\n",
    "for model_size in list(all_models.keys()):\n",
    "    if \"model\" in all_models[model_size]:\n",
    "        del all_models[model_size][\"model\"]\n",
    "        print(f\"  ✓ Deleted encoder model: {model_size}\")\n",
    "\n",
    "# Run garbage collection\n",
    "gc.collect()\n",
    "\n",
    "# Clear GPU cache\n",
    "if device.type == \"mps\":\n",
    "    torch.mps.empty_cache()\n",
    "elif device.type == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"✓ Cleanup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6dtx4zcrwlo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ decode_centroid function defined\n"
     ]
    }
   ],
   "source": [
    "# Import necessary class for encoder outputs\n",
    "from transformers.modeling_outputs import BaseModelOutput\n",
    "\n",
    "def decode_centroid(centroid_vec, decoder_model, tokenizer, encoder_model, task_prompt=\"describe this factor:\"):\n",
    "    \"\"\"\n",
    "    Feeds a centroid (numpy array) into the decoder as encoder hidden state.\n",
    "\n",
    "    The centroid is appended to the task prompt's encoder embeddings to condition the generation.\n",
    "\n",
    "    Args:\n",
    "        centroid_vec: numpy array of shape (hidden_dim,)\n",
    "        decoder_model: AutoModelForSeq2SeqLM instance\n",
    "        tokenizer: tokenizer for the model\n",
    "        encoder_model: encoder model to encode the task prompt\n",
    "        task_prompt: text prompt to condition the generation\n",
    "\n",
    "    Returns:\n",
    "        str: decoded label text\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Encode the task prompt using the encoder\n",
    "        prompt_inputs = tokenizer(task_prompt, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        prompt_encoder_outputs = encoder_model.encoder(**prompt_inputs)\n",
    "        prompt_embeddings = prompt_encoder_outputs.last_hidden_state  # shape: (1, prompt_seq_len, hidden_dim)\n",
    "\n",
    "        # Convert centroid to proper encoder hidden-state tensor\n",
    "        centroid_tensor = torch.tensor(centroid_vec, dtype=torch.float32, device=device)\n",
    "        centroid_tensor = centroid_tensor.unsqueeze(0).unsqueeze(0)  # shape: (batch=1, seq_len=1, hidden_dim)\n",
    "\n",
    "        # Concatenate prompt embeddings + centroid along sequence dimension\n",
    "        combined_embeddings = torch.cat([prompt_embeddings, centroid_tensor], dim=1)  # shape: (1, prompt_seq_len+1, hidden_dim)\n",
    "\n",
    "        # Create attention mask for the combined sequence\n",
    "        attention_mask = torch.ones(combined_embeddings.shape[:2], dtype=torch.long, device=device)  # shape: (1, seq_len)\n",
    "\n",
    "        # Wrap into BaseModelOutput dataclass expected by model.generate()\n",
    "        encoder_outputs = BaseModelOutput(last_hidden_state=combined_embeddings)\n",
    "\n",
    "        # Create proper decoder_input_ids (T5 models need this to start generation)\n",
    "        # Use pad_token_id as the starting token (standard for T5)\n",
    "        decoder_start_token_id = decoder_model.config.decoder_start_token_id or tokenizer.pad_token_id\n",
    "        decoder_input_ids = torch.tensor([[decoder_start_token_id]], dtype=torch.long, device=device)\n",
    "\n",
    "        # Generate label text\n",
    "        outputs = decoder_model.generate(\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            max_new_tokens=10,\n",
    "            num_beams=5,\n",
    "            do_sample=False,\n",
    "            temperature=1.0,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "        # Decode output text\n",
    "        decoded_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return decoded_text.strip()\n",
    "\n",
    "print(\"✓ decode_centroid function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "t8bgxeb18om",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding factor centroids into text labels...\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "PREFIXLM Model - Decoding Centroids\n",
      "======================================================================\n",
      "\n",
      "Decoding centroid for factor: Behavioral\n",
      "  Predicted Label: \n",
      "\n",
      "Decoding centroid for factor: Emotional\n",
      "  Predicted Label: \n",
      "\n",
      "======================================================================\n",
      "✓ Decoding complete for all models!\n",
      "======================================================================\n",
      "\n",
      "Generated Factor Labels Summary:\n",
      "======================================================================\n",
      "\n",
      "PREFIXLM Model:\n",
      "  Behavioral   → \n",
      "  Emotional    → \n"
     ]
    }
   ],
   "source": [
    "# Generate labels for all factor centroids across all models\n",
    "print(\"Decoding factor centroids into text labels...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Dictionary to store factor labels for all models\n",
    "all_factor_labels = {}\n",
    "\n",
    "# Loop through each enabled model\n",
    "for model_size in sorted(all_decoder_models.keys()):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"{model_size} Model - Decoding Centroids\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Get components for this model\n",
    "    decoder_model = all_decoder_models[model_size]\n",
    "    encoder_model = all_models[model_size][\"model\"]\n",
    "    tokenizer = all_models[model_size][\"tokenizer\"]\n",
    "    centroids = all_centroids[model_size]\n",
    "    \n",
    "    # Initialize dictionary for this model's labels\n",
    "    factor_labels = {}\n",
    "    \n",
    "    # Decode each factor centroid\n",
    "    for factor, centroid_vec in sorted(centroids.items()):\n",
    "        print(f\"\\nDecoding centroid for factor: {factor}\")\n",
    "        \n",
    "        try:\n",
    "            # Decode the centroid\n",
    "            label = decode_centroid(\n",
    "                centroid_vec=centroid_vec,\n",
    "                decoder_model=decoder_model,\n",
    "                tokenizer=tokenizer,\n",
    "                encoder_model=encoder_model,\n",
    "                task_prompt=\"describe this factor:\"\n",
    "            )\n",
    "            \n",
    "            # Store the label\n",
    "            factor_labels[factor] = label\n",
    "            \n",
    "            print(f\"  Predicted Label: {label}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error decoding centroid: {type(e).__name__}: {str(e)}\")\n",
    "            factor_labels[factor] = \"[ERROR]\"\n",
    "    \n",
    "    # Store labels for this model\n",
    "    all_factor_labels[model_size] = factor_labels\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"✓ Decoding complete for all models!\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Show summary\n",
    "print(\"\\nGenerated Factor Labels Summary:\")\n",
    "print(\"=\" * 70)\n",
    "for model_size in sorted(all_factor_labels.keys()):\n",
    "    print(f\"\\n{model_size} Model:\")\n",
    "    for factor, label in sorted(all_factor_labels[model_size].items()):\n",
    "        print(f\"  {factor:12s} → {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wbk5kvw3vg",
   "metadata": {},
   "source": [
    "## Generate Factor Labels from Actual Item Text (Text-Based Approach)\n",
    "\n",
    "This alternative approach generates factor labels using the actual item text rather than feeding centroid vectors to the decoder.\n",
    "\n",
    "**Approach:**\n",
    "- For each factor, concatenate all the items that belong to that factor\n",
    "- Pass this concatenated text through the encoder normally\n",
    "- Use the decoder to generate a short descriptive label\n",
    "- This uses the model as it was designed: text-in, text-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2yhcod1dblj",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ generate_label_from_items function defined\n"
     ]
    }
   ],
   "source": [
    "def generate_label_from_items(factor_items, decoder_model, tokenizer, task_prompt):\n",
    "    \"\"\"\n",
    "    Generate a factor label from actual item text using text-to-text generation.\n",
    "    \n",
    "    Args:\n",
    "        factor_items: list of strings (the items belonging to this factor)\n",
    "        decoder_model: AutoModelForSeq2SeqLM instance\n",
    "        tokenizer: tokenizer for the model\n",
    "        task_prompt: instruction prompt for the model\n",
    "        \n",
    "    Returns:\n",
    "        str: generated label text\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Concatenate items with separators\n",
    "        items_text = \" | \".join(factor_items)\n",
    "        \n",
    "        # Create full input text\n",
    "        input_text = f\"{task_prompt} {items_text}\"\n",
    "        \n",
    "        # Tokenize input (truncate if too long)\n",
    "        inputs = tokenizer(\n",
    "            input_text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512  # Limit input length\n",
    "        ).to(device)\n",
    "        \n",
    "        # Generate label text\n",
    "        outputs = decoder_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=10,\n",
    "            num_beams=5,\n",
    "            do_sample=False,\n",
    "            temperature=1.0,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        \n",
    "        # Decode output text\n",
    "        decoded_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return decoded_text.strip()\n",
    "\n",
    "print(\"✓ generate_label_from_items function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wwtorqj7jbi",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating factor labels from actual item text...\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "PREFIXLM Model - Text-Based Label Generation\n",
      "======================================================================\n",
      "\n",
      "Generating label for factor: Behavioral\n",
      "  Number of items: 16\n",
      "['I avoid tasks that I know I need to do.', 'I postpone tasks until the last minute.', \"I find myself saying 'I'll do it tomorrow.'\", 'I have difficulty managing my time for assignments.', 'I find that tasks take longer than expected.', 'I put off starting tasks when they are important.', 'I spend time on unimportant tasks when I have more urgent tasks to complete.', 'I have difficulty prioritizing tasks.', 'I put off starting tasks, even when I have plenty of time to do them.', 'I postpone tasks until the last minute, even when I have no valid reason to wait.', 'I find myself engaging in unrelated activities when I should be focusing on my main task.', 'When I have a clear plan, I still delay getting started for no real reason.', 'I delay important tasks even though I know it will create unnecessary stress later.', 'I have postponed a deadline knowing that it would negatively affect my academic performance.', 'I put off work until the last minute when I know it will cause me to rush to finish.', 'I stay up late to complete assignments on time.']\n",
      "  Generated Label: I avoid tasks that I know I need to do\n",
      "\n",
      "Generating label for factor: Emotional\n",
      "  Number of items: 14\n",
      "['When I am fully aware that delaying studying will hurt my grades, I still do it.', 'I rush to complete tasks right before the deadline, causing me to miss out on social activities.', 'I negatively compare myself to others who manage their time better.', 'I lose sleep because I am anxious about unfinished tasks that I should have done.', 'I feel guilty when I realize I could have done a better job on a task if I had started earlier.', 'I am mentally exhausted because I worry about the work I’ve been avoiding.', 'I underestimate how long a task will take, leading to unnecessary stress.', 'It is hard to enjoy other activities because I feel guilty about tasks I’m avoiding.', 'I regret not putting in more effort on tasks because I delayed starting them.', 'I am physically tense or uneasy when I think about tasks I’ve been avoiding.', 'When my workload piles up it makes it harder for me to focus.', 'When I leave tasks to the last minute, I feel a sense of dread about how much work I have left to do.', 'When I complete a task, I am disappointed in myself if I know I didn’t give it my best effort.', 'When people expect me to do well, I avoid starting tasks because I’m afraid I won’t meet their standards.']\n",
      "  Generated Label: When I am fully aware that delaying studying will hurt\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Generate labels from actual item text for all factors across all models\n",
    "print(\"Generating factor labels from actual item text...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Dictionary to store text-based factor labels for all models\n",
    "text_based_factor_labels = {}\n",
    "\n",
    "# Loop through each enabled model\n",
    "for model_size in sorted(all_decoder_models.keys()):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"{model_size} Model - Text-Based Label Generation\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Get components for this model\n",
    "    decoder_model = all_decoder_models[model_size]\n",
    "    tokenizer = all_models[model_size][\"tokenizer\"]\n",
    "    \n",
    "    # Initialize dictionary for this model's labels\n",
    "    factor_labels = {}\n",
    "    \n",
    "    # Generate label for each factor\n",
    "    for factor in sorted(unique_factors):\n",
    "        print(f\"\\nGenerating label for factor: {factor}\")\n",
    "        \n",
    "        # Get all items belonging to this factor\n",
    "        factor_items = [items[i] for i, f in enumerate(factors) if f == factor]\n",
    "        print(f\"  Number of items: {len(factor_items)}\")\n",
    "\n",
    "        print(factor_items)\n",
    "        \n",
    "        try:\n",
    "            # Generate the label from concatenated items\n",
    "            label = generate_label_from_items(\n",
    "                factor_items=factor_items,\n",
    "                decoder_model=decoder_model,\n",
    "                tokenizer=tokenizer,\n",
    "                task_prompt=\"Given this list of input text, the common ONE-WORD theme is:\"\n",
    "            )\n",
    "            \n",
    "            # Store the label\n",
    "            factor_labels[factor] = label\n",
    "            \n",
    "            print(f\"  Generated Label: {label}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error generating label: {type(e).__name__}: {str(e)}\")\n",
    "            factor_labels[factor] = \"[ERROR]\"\n",
    "    \n",
    "    # Store labels for this model\n",
    "    text_based_factor_labels[model_size] = factor_labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
